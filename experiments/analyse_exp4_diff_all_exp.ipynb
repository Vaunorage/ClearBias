{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T04:15:21.455357Z",
     "start_time": "2024-11-29T04:14:49.791063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from path import HERE\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def calculate_distances(synth_df: pd.DataFrame, result_df: pd.DataFrame, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Calculate distances between synthetic and result features.\"\"\"\n",
    "    # Ensure both dataframes have the required feature columns\n",
    "    valid_features = []\n",
    "    for col in feature_cols:\n",
    "        if col in synth_df.columns and col in result_df.columns:\n",
    "            valid_features.append(col)\n",
    "\n",
    "    if not valid_features:\n",
    "        raise ValueError(f\"No matching feature columns found between synthetic and result data\")\n",
    "\n",
    "    # Use only valid features\n",
    "    synth_features = synth_df[valid_features].to_numpy()\n",
    "    result_features = result_df[valid_features].to_numpy()\n",
    "\n",
    "    distance_matrix = np.linalg.norm(\n",
    "        synth_features[:, np.newaxis, :] - result_features[np.newaxis, :, :],\n",
    "        axis=2\n",
    "    )\n",
    "\n",
    "    synth_index_map = {idx: i for i, idx in enumerate(synth_df.index)}\n",
    "    result_index_map = {idx: i for i, idx in enumerate(result_df.index)}\n",
    "\n",
    "    distance_data = []\n",
    "    for group_key in synth_df[\"group_key\"].unique():\n",
    "        group_mask = synth_df[\"group_key\"] == group_key\n",
    "        synth_indices = synth_df[group_mask].index\n",
    "\n",
    "        for synth_idx in synth_indices:\n",
    "            matrix_synth_idx = synth_index_map[synth_idx]\n",
    "\n",
    "            for result_idx in result_df.index:\n",
    "                matrix_result_idx = result_index_map[result_idx]\n",
    "                distance_data.append({\n",
    "                    \"group_key\": group_key,\n",
    "                    \"synth_df_couple_key\": synth_df.at[synth_idx, \"couple_key\"],\n",
    "                    \"result_df_couple_key\": result_df.at[result_idx, \"couple_key\"],\n",
    "                    \"distance\": distance_matrix[matrix_synth_idx, matrix_result_idx],\n",
    "                    \"experiment_id\": synth_df['experiment_id'].iloc[0]\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(distance_data)\n",
    "\n",
    "\n",
    "def prepare_result_combinations(df: pd.DataFrame, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Prepare combinations of features for analysis.\"\"\"\n",
    "    # Validate feature columns exist in dataframe\n",
    "    valid_features = [col for col in feature_cols if col in df.columns]\n",
    "    if not valid_features:\n",
    "        raise ValueError(\"No valid feature columns found in data\")\n",
    "\n",
    "    all_combinations = []\n",
    "\n",
    "    for couple_key in df['couple_key'].unique():\n",
    "        indivs = couple_key.split('-')\n",
    "        sorted_indivs = sort_two_strings(indivs[0], indivs[1])\n",
    "        sorted_couple_key = f\"{sorted_indivs[0]}-{sorted_indivs[1]}\"\n",
    "\n",
    "        indiv1_data = df[df['indv_key'] == sorted_indivs[0]]\n",
    "        indiv2_data = df[df['indv_key'] == sorted_indivs[1]]\n",
    "        couple_data = pd.concat([indiv1_data, indiv2_data])\n",
    "\n",
    "        if couple_data.shape[0] == 2:\n",
    "            continue\n",
    "\n",
    "        is_part_of_group = couple_data['is_couple_part_of_a_group'].iloc[0] != '0'\n",
    "        unique_individuals = couple_data[valid_features].drop_duplicates().values\n",
    "        pairs = list(combinations(range(len(unique_individuals)), 2))\n",
    "\n",
    "        for i, j in pairs:\n",
    "            combination = {\n",
    "                'couple_key': sorted_couple_key,\n",
    "                'is_part_of_group': is_part_of_group\n",
    "            }\n",
    "\n",
    "            indiv1_features = unique_individuals[i]\n",
    "            indiv2_features = unique_individuals[j]\n",
    "\n",
    "            for idx, feat in enumerate(valid_features):\n",
    "                combination[f'{feat}_1'] = indiv1_features[idx]\n",
    "                combination[f'{feat}_2'] = indiv2_features[idx]\n",
    "\n",
    "            all_combinations.append(combination)\n",
    "\n",
    "    return pd.DataFrame(all_combinations)\n",
    "\n",
    "\n",
    "def load_experiment_data(conn, experiment_id: str) -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"Load synthetic and results data for a given experiment.\"\"\"\n",
    "    # Load synthetic data\n",
    "    df_synth = pd.read_sql_query(\n",
    "        f\"SELECT experiment_id, full_data FROM synthetic_data where experiment_id='{experiment_id}'\",\n",
    "        conn\n",
    "    )\n",
    "    df_synth = pd.DataFrame(json.loads(df_synth['full_data'].iloc[0]))\n",
    "    df_synth['experiment_id'] = experiment_id\n",
    "\n",
    "    # Load results data\n",
    "    df_result = pd.read_sql_query(\n",
    "        f\"\"\"SELECT * FROM augmented_results ar\n",
    "        left join main.analysis_metadata am on am.analysis_id=ar.analysis_id\n",
    "        where experiment_id='{experiment_id}'\"\"\",\n",
    "        conn\n",
    "    )\n",
    "    df_result_data = pd.DataFrame(list(df_result['data'].apply(json.loads)))\n",
    "    df_result = pd.concat([df_result.reset_index(drop=True), df_result_data.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Get all potential feature columns\n",
    "    synth_features = [col for col in df_synth.columns if 'Attr' in col]\n",
    "    result_features = [col for col in df_result.columns if 'Attr' in col]\n",
    "\n",
    "    # Find common features\n",
    "    feature_cols = list(set(synth_features) & set(result_features))\n",
    "\n",
    "    if not feature_cols:\n",
    "        raise ValueError(f\"No common feature columns found between synthetic and result data\")\n",
    "\n",
    "    return df_synth, df_result, feature_cols\n",
    "\n",
    "\n",
    "def sort_two_strings(str1: str, str2: str) -> Tuple[str, str]:\n",
    "    \"\"\"Sort two strings lexicographically.\"\"\"\n",
    "    return (str1, str2) if str1 <= str2 else (str2, str1)\n",
    "\n",
    "\n",
    "def prepare_result_combinations(df: pd.DataFrame, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Prepare combinations of features for analysis.\"\"\"\n",
    "    # Validate feature columns exist in dataframe\n",
    "    valid_features = [col for col in feature_cols if col in df.columns]\n",
    "    if not valid_features:\n",
    "        raise ValueError(\"No valid feature columns found in data\")\n",
    "\n",
    "    all_combinations = []\n",
    "\n",
    "    for couple_key in df['couple_key'].unique():\n",
    "        indivs = couple_key.split('-')\n",
    "        sorted_indivs = sort_two_strings(indivs[0], indivs[1])\n",
    "        sorted_couple_key = f\"{sorted_indivs[0]}-{sorted_indivs[1]}\"\n",
    "\n",
    "        indiv1_data = df[df['indv_key'] == sorted_indivs[0]]\n",
    "        indiv2_data = df[df['indv_key'] == sorted_indivs[1]]\n",
    "        couple_data = pd.concat([indiv1_data, indiv2_data])\n",
    "\n",
    "        if couple_data.shape[0] == 2:\n",
    "            continue\n",
    "\n",
    "        is_part_of_group = couple_data['is_couple_part_of_a_group'].iloc[0] != '0'\n",
    "        unique_individuals = couple_data[valid_features].drop_duplicates().values\n",
    "        pairs = list(combinations(range(len(unique_individuals)), 2))\n",
    "\n",
    "        for i, j in pairs:\n",
    "            combination = {\n",
    "                'couple_key': sorted_couple_key,\n",
    "                'is_part_of_group': is_part_of_group\n",
    "            }\n",
    "\n",
    "            indiv1_features = unique_individuals[i]\n",
    "            indiv2_features = unique_individuals[j]\n",
    "\n",
    "            for idx, feat in enumerate(valid_features):\n",
    "                combination[f'{feat}_1'] = indiv1_features[idx]\n",
    "                combination[f'{feat}_2'] = indiv2_features[idx]\n",
    "\n",
    "            all_combinations.append(combination)\n",
    "\n",
    "    return pd.DataFrame(all_combinations)\n",
    "\n",
    "\n",
    "def calculate_distances(synth_df: pd.DataFrame, result_df: pd.DataFrame, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Calculate distances between synthetic and result features.\"\"\"\n",
    "    # Ensure both dataframes have the required feature columns\n",
    "    valid_features = []\n",
    "    for col in feature_cols:\n",
    "        if col in synth_df.columns and col in result_df.columns:\n",
    "            valid_features.append(col)\n",
    "\n",
    "    if not valid_features:\n",
    "        raise ValueError(f\"No matching feature columns found between synthetic and result data\")\n",
    "\n",
    "    # Use only valid features\n",
    "    synth_features = synth_df[valid_features].to_numpy()\n",
    "    result_features = result_df[valid_features].to_numpy()\n",
    "\n",
    "    distance_matrix = np.linalg.norm(\n",
    "        synth_features[:, np.newaxis, :] - result_features[np.newaxis, :, :],\n",
    "        axis=2\n",
    "    )\n",
    "\n",
    "    synth_index_map = {idx: i for i, idx in enumerate(synth_df.index)}\n",
    "    result_index_map = {idx: i for i, idx in enumerate(result_df.index)}\n",
    "\n",
    "    distance_data = []\n",
    "    for group_key in synth_df[\"group_key\"].unique():\n",
    "        group_mask = synth_df[\"group_key\"] == group_key\n",
    "        synth_indices = synth_df[group_mask].index\n",
    "\n",
    "        for synth_idx in synth_indices:\n",
    "            matrix_synth_idx = synth_index_map[synth_idx]\n",
    "\n",
    "            for result_idx in result_df.index:\n",
    "                matrix_result_idx = result_index_map[result_idx]\n",
    "                distance_data.append({\n",
    "                    \"group_key\": group_key,\n",
    "                    \"synth_df_couple_key\": synth_df.at[synth_idx, \"couple_key\"],\n",
    "                    \"result_df_couple_key\": result_df.at[result_idx, \"couple_key\"],\n",
    "                    \"distance\": distance_matrix[matrix_synth_idx, matrix_result_idx],\n",
    "                    \"experiment_id\": synth_df['experiment_id'].iloc[0]\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(distance_data)\n",
    "\n",
    "\n",
    "def process_experiment(conn, experiment_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Process a single experiment and return the correlation results.\"\"\"\n",
    "    # Load data\n",
    "    df_synth, df_result, feature_cols = load_experiment_data(conn, experiment_id)\n",
    "\n",
    "    # Generate combinations\n",
    "    synth_combinations_df = df_synth  # Assuming DiscriminationData.generate_individual_synth_combinations is not available\n",
    "    result_combinations_df = prepare_result_combinations(df_result, feature_cols)\n",
    "\n",
    "    # Calculate distances\n",
    "    distances_df = calculate_distances(synth_combinations_df, result_combinations_df, feature_cols)\n",
    "\n",
    "    # Add is_part_of_group if missing\n",
    "    if \"is_part_of_group\" not in result_combinations_df.columns:\n",
    "        result_combinations_df[\"is_part_of_group\"] = False\n",
    "\n",
    "    # Merge with result data\n",
    "    distances_df = distances_df.merge(\n",
    "        result_combinations_df[[\"couple_key\", \"is_part_of_group\"]].rename(\n",
    "            columns={\"couple_key\": \"result_df_couple_key\"}\n",
    "        ),\n",
    "        on=\"result_df_couple_key\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Calculate aggregated statistics\n",
    "    agg_stats = distances_df.groupby([\"group_key\", \"is_part_of_group\", \"experiment_id\"])[\"distance\"].agg(\n",
    "        min_distance=\"min\",\n",
    "        max_distance=\"max\",\n",
    "        median_distance=\"median\",\n",
    "        mean_distance=\"mean\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # Get calculated columns from synthetic data\n",
    "    calculated_columns = [col for col in df_synth.columns if col.startswith('calculated')]\n",
    "    aggregated_metrics = df_synth.groupby('group_key')[calculated_columns].agg(['mean', 'min', 'max', 'median'])\n",
    "    aggregated_metrics.columns = ['_'.join(col).strip() for col in aggregated_metrics.columns.values]\n",
    "    aggregated_metrics = aggregated_metrics.reset_index()\n",
    "\n",
    "    # Merge all data\n",
    "    final_df = agg_stats.merge(aggregated_metrics, on='group_key', how='left')\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def analyze_all_experiments(db_path: str) -> None:\n",
    "    \"\"\"Analyze all experiments in the database and create visualization.\"\"\"\n",
    "    conn = create_engine(f'sqlite:///{db_path}')\n",
    "\n",
    "    # Get all experiment IDs\n",
    "    experiment_ids = pd.read_sql_query(\"SELECT DISTINCT experiment_id FROM synthetic_data\", conn)['experiment_id']\n",
    "\n",
    "    # Process all experiments\n",
    "    all_results = []\n",
    "    for exp_id in experiment_ids:\n",
    "        try:\n",
    "            exp_results = process_experiment(conn, exp_id)\n",
    "            all_results.append(exp_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing experiment {exp_id}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Combine all results\n",
    "    combined_results = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    # Calculate correlations\n",
    "    distance_columns = [\"min_distance\", \"max_distance\", \"mean_distance\", \"median_distance\"]\n",
    "    calculated_columns = [col for col in combined_results.columns if col.startswith(\"calculated\")]\n",
    "    correlations = combined_results[calculated_columns + distance_columns].corr()\n",
    "    mean_correlations = correlations.loc[\n",
    "        list(filter(lambda x: 'mean' in x, correlations.index)),\n",
    "        ['mean_distance', 'median_distance']\n",
    "    ]\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    y_pos = np.arange(len(mean_correlations.index))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.barh(y_pos - width / 2, mean_correlations['mean_distance'], width,\n",
    "             label='Mean Distance', color='#2196F3', alpha=0.7)\n",
    "    plt.barh(y_pos + width / 2, mean_correlations['median_distance'], width,\n",
    "             label='Median Distance', color='#FF5722', alpha=0.7)\n",
    "\n",
    "    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.xlabel('Correlation Value', fontsize=12)\n",
    "    plt.ylabel('Metrics', fontsize=12)\n",
    "    plt.title('Distance Correlation Comparison Across All Experiments', fontsize=14, pad=20)\n",
    "    plt.yticks(y_pos, mean_correlations.index, fontsize=10)\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    DB_PATH = HERE.joinpath('experiments/discrimination_detection_results5.db')\n",
    "    analyze_all_experiments(DB_PATH)"
   ],
   "id": "65288a3acb8e0e79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing experiment 9f5b916c-6314-49ab-9288-eac39c633bad: No common feature columns found between synthetic and result data\n",
      "Error processing experiment 489b23cc-02be-41b3-98d3-c24e0a256c22: No common feature columns found between synthetic and result data\n",
      "Error processing experiment 0978edd3-1cb1-4ead-ae08-7e831c7406e3: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment 64e0e40c-c45d-47af-bf99-15b58648c572: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment e0d82bbe-d219-40cc-8946-a23f2c06ff82: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment ecaec97f-cd7d-4ac5-af71-0505c7291240: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment 803fdf6b-9e48-4503-b39e-35e32bbbba10: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment 74a9c770-1863-4726-84b1-36e6fe00e87a: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment 6029875d-73dd-4d5d-91cb-babbc9e77ccf: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment 16586854-8c4b-47df-bfb7-526be75ec51c: No common feature columns found between synthetic and result data\n",
      "Error processing experiment 7ea48886-8e96-471f-8357-ba7da570f38c: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment bec77949-bf08-4654-8280-b02a6aad354c: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment 4ff22f7c-61fe-4ffa-ab12-56abbbe952fc: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment 2cffcd0b-3593-4dc7-9105-45139bf55cf3: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment b46f3692-b6a4-47fa-ab45-d796faa46b13: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment e95602d5-46b3-4c8c-a49b-20421a2d5b43: No matching feature columns found between synthetic and result data\n",
      "Error processing experiment af07cbad-8ab3-4465-945b-5765b3684095: No matching feature columns found between synthetic and result data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_46636\\2908659060.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m    316\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[1;31m# Usage example:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    318\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"__main__\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    319\u001B[0m     \u001B[0mDB_PATH\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mHERE\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoinpath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'experiments/discrimination_detection_results5.db'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 320\u001B[1;33m     \u001B[0manalyze_all_experiments\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDB_PATH\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_46636\\2908659060.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(db_path)\u001B[0m\n\u001B[0;32m    276\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mexp_id\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mexperiment_ids\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    277\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    278\u001B[0m             \u001B[0mexp_results\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprocess_experiment\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mexp_id\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    279\u001B[0m             \u001B[0mall_results\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexp_results\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 280\u001B[1;33m         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    281\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33mf\"\u001B[0m\u001B[1;33mError processing experiment \u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mexp_id\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m: \u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    282\u001B[0m             \u001B[1;32mcontinue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    283\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_46636\\2908659060.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(conn, experiment_id)\u001B[0m\n\u001B[0;32m    226\u001B[0m     \u001B[0mdf_synth\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf_result\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeature_cols\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_experiment_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mexperiment_id\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    227\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    228\u001B[0m     \u001B[1;31m# Generate combinations\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    229\u001B[0m     \u001B[0msynth_combinations_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf_synth\u001B[0m  \u001B[1;31m# Assuming DiscriminationData.generate_individual_synth_combinations is not available\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 230\u001B[1;33m     \u001B[0mresult_combinations_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprepare_result_combinations\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf_result\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeature_cols\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    231\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    232\u001B[0m     \u001B[1;31m# Calculate distances\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    233\u001B[0m     \u001B[0mdistances_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalculate_distances\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msynth_combinations_df\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult_combinations_df\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeature_cols\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_46636\\2908659060.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(df, feature_cols)\u001B[0m\n\u001B[0;32m    146\u001B[0m         \u001B[0mindivs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcouple_key\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'-'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    147\u001B[0m         \u001B[0msorted_indivs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msort_two_strings\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindivs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindivs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    148\u001B[0m         \u001B[0msorted_couple_key\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33mf\"\u001B[0m\u001B[1;33m{\u001B[0m\u001B[0msorted_indivs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;33m{\u001B[0m\u001B[0msorted_indivs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 150\u001B[1;33m         \u001B[0mindiv1_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'indv_key'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0msorted_indivs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    151\u001B[0m         \u001B[0mindiv2_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'indv_key'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0msorted_indivs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m         \u001B[0mcouple_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mindiv1_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindiv2_data\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\ClearBias\\venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4089\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwhere\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4090\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4091\u001B[0m         \u001B[1;31m# Do we have a (boolean) 1d indexer?\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4092\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_bool_indexer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4093\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_bool_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4094\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4095\u001B[0m         \u001B[1;31m# We are left with two options: a single key, and a collection of keys,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4096\u001B[0m         \u001B[1;31m# We interpret tuples as collections only for non-MultiIndex\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\ClearBias\\venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4150\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4151\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4152\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdeep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4153\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4154\u001B[1;33m         \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnonzero\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4155\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_take_with_is_copy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f572d1d5bd37276c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
