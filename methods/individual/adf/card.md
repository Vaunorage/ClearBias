### Title : White-box fairness testing through adversarial sampling 
### Metric: What metric does the method rely on to find discrimination?

The method, called Adversarial Discrimination Finder (ADF), focuses on finding **individual discrimination**.

*   **Definition**: The paper formally defines an individual discriminatory instance in Definition 2.1 (page 5). An instance `x` is considered an individual discriminatory instance if another instance `x'` exists that meets the following criteria:
    1.  `x'` differs from `x` only in one or more **protected attributes** (e.g., race, gender).
    2.  All **non-protected attributes** remain the same.
    3.  The model's prediction for `x` is different from its prediction for `x'` (i.e., `D(x) â‰  D(x')`).

The method aims to generate these `(x, x')` pairs, which are called "individual discriminatory instance pairs."

### Location: Where does this method try to find discrimination?

The method finds discrimination **within the DNN model** by generating new, adversarial data samples.

*   It is a **white-box** approach, meaning it requires full access to the model's internal structure and parameters.
*   It uses the original dataset as a starting point ("seed instances").
*   It then systematically perturbs these seeds using gradient information from the model to generate new data instances that are not in the original dataset. The goal of these perturbations is to find inputs that cross the model's decision boundary when only a protected attribute is changed, thus revealing discrimination in the model's logic.

### What they find: What exactly does this method try to find and what does it return?

*   **What it finds**: The method's primary goal is to find and generate **individual discriminatory instances**. It does not focus on group-level discrimination (i.e., comparing the outcomes of one entire protected group against another) but on finding specific pairs of individuals who are treated unfairly by the model.

*   **What it returns**: The method returns a **set of individual discriminatory instances**. These are data points (represented as feature vectors) that have been proven to cause a discriminatory outcome from the model. As stated in the algorithm descriptions (page 6 and 8), the functions `return g_id` and `return l_id`, which are sets of these identified instances.

### Performance: How has this method's performance been evaluated and what was the result?

The performance of ADF was evaluated against two state-of-the-art methods, AEQUITAS and SG, across three datasets. The evaluation was based on effectiveness, efficiency, and the usefulness of the generated instances.

*   **Effectiveness (Finding more instances)**:
    *   Compared to AEQUITAS, ADF generated **25 times more** individual discriminatory instances and explored the input space 9 times more effectively.
    *   Compared to SG (given the same time limit), ADF generated **6.5 times more** individual discriminatory instances.
    *   The "success rate" (the ratio of discriminatory instances found to instances generated) of ADF was over twice that of AEQUITAS (40.89% vs 18.22%).

*   **Efficiency (Finding instances faster)**:
    *   ADF was significantly faster than the baselines. The paper reports an "average speedup of **104%** (over 2x faster) compared to AEQUITAS and **588%** (over 6x faster) compared to SG" for generating the same number of discriminatory instances (Answer to RQ2, page 11).

*   **Usefulness (Improving model fairness)**:
    *   The discriminatory instances generated by ADF were used to augment the training data and retrain the model. This process significantly improved the model's fairness.
    *   Retraining with ADF-generated data led to an average fairness improvement of **57.2%**, outperforming the improvements from AEQUITAS (45.1%) and SG (49.1%) (Answer to RQ3, page 12).