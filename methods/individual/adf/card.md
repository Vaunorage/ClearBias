### Title : White-box fairness testing through adversarial sampling 
### Metric: What metric does the method rely on to find discrimination?

The method, called Adversarial Discrimination Finder (ADF), focuses on finding **individual discrimination**.

*   **Definition**: The paper formally defines an individual discriminatory instance in Definition 2.1 (page 5). An instance `x` is considered an individual discriminatory instance if another instance `x'` exists that meets the following criteria:
    1.  `x'` differs from `x` only in one or more **protected attributes** (e.g., race, gender).
    2.  All **non-protected attributes** remain the same.
    3.  The model's prediction for `x` is different from its prediction for `x'` (i.e., `D(x) â‰  D(x')`).

The method aims to generate these `(x, x')` pairs, which are called "individual discriminatory instance pairs."

### Location: Where does this method try to find discrimination?

The method finds discrimination **within the DNN model** by generating new, adversarial data samples.

*   It is a **white-box** approach, meaning it requires full access to the model's internal structure and parameters.
*   It uses the original dataset as a starting point ("seed instances").
*   It then systematically perturbs these seeds using gradient information from the model to generate new data instances that are not in the original dataset. The goal of these perturbations is to find inputs that cross the model's decision boundary when only a protected attribute is changed, thus revealing discrimination in the model's logic.

### What they find: What exactly does this method try to find and what does it return?

*   **What it finds**: The method's primary goal is to find and generate **individual discriminatory instances**. It does not focus on group-level discrimination (i.e., comparing the outcomes of one entire protected group against another) but on finding specific pairs of individuals who are treated unfairly by the model.

*   **What it returns**: The method returns a **set of individual discriminatory instances**. These are data points (represented as feature vectors) that have been proven to cause a discriminatory outcome from the model. As stated in the algorithm descriptions (page 6 and 8), the functions `return g_id` and `return l_id`, which are sets of these identified instances.

### Performance: How has this method's performance been evaluated and what was the result?

The performance of ADF was evaluated against two state-of-the-art methods, AEQUITAS and SG, across three datasets. The evaluation was based on effectiveness, efficiency, and the usefulness of the generated instances.

*   **Effectiveness (Finding more instances)**:
    *   Compared to AEQUITAS, ADF generated **25 times more** individual discriminatory instances and explored the input space 9 times more effectively.
    *   Compared to SG (given the same time limit), ADF generated **6.5 times more** individual discriminatory instances.
    *   The "success rate" (the ratio of discriminatory instances found to instances generated) of ADF was over twice that of AEQUITAS (40.89% vs 18.22%).

*   **Efficiency (Finding instances faster)**:
    *   ADF was significantly faster than the baselines. The paper reports an "average speedup of **104%** (over 2x faster) compared to AEQUITAS and **588%** (over 6x faster) compared to SG" for generating the same number of discriminatory instances (Answer to RQ2, page 11).

*   **Usefulness (Improving model fairness)**:
    *   The discriminatory instances generated by ADF were used to augment the training data and retrain the model. This process significantly improved the model's fairness.
    *   Retraining with ADF-generated data led to an average fairness improvement of **57.2%**, outperforming the improvements from AEQUITAS (45.1%) and SG (49.1%) (Answer to RQ3, page 12).

# Adversarial Debiasing Framework (ADF) Documentation

## Implementation

The Adversarial Debiasing Framework (ADF) is an algorithm designed to detect and identify discriminatory instances in machine learning models. It focuses on finding pairs of inputs that differ only in their protected attributes but receive different predictions from the model, which indicates potential bias.

## Input

The ADF algorithm takes the following inputs:

### Primary Input
- **data** (DiscriminationData object): Contains the dataset and metadata including:
  - `dataframe`: The complete dataset
  - `training_dataframe`: Data used for training the model
  - `xdf`: Features dataframe used for testing
  - `protected_attributes`: List of column names that are considered protected attributes (e.g., race, gender)
  - `sensitive_indices`: Indices of the protected attributes in the feature array
  - `input_bounds`: Min/max bounds for each feature
  - `outcome_column`: Name of the target variable

### Configuration Parameters
- **max_global** (int, default=2000): Maximum number of samples for the global search phase
- **max_local** (int, default=2000): Maximum number of samples for the local search phase
- **cluster_num** (int, default=10): Number of clusters to divide the input data into
- **random_seed** (int, default=42): Random seed for reproducibility
- **max_runtime_seconds** (int, default=3600): Maximum runtime in seconds before early termination
- **max_tsn** (int, default=None): Maximum number of test samples to generate before termination
- **step_size** (float, default=0.4): Step size for perturbation in both global and local search phases
- **one_attr_at_a_time** (bool, default=False): If True, only one protected attribute is varied at a time
- **db_path** (str, default=None): Path to database for storing results
- **analysis_id** (str, default=None): ID for the current analysis run
- **use_cache** (bool, default=True): Whether to use cached models

## Algorithm Process

The ADF algorithm works in two main phases:

### Global Search Phase
- Clusters the input data to find diverse starting points
- For each starting point, applies global perturbation to find discriminatory instances
- Adds discriminatory instances to a set for the local search phase

### Local Search Phase
- Uses the basin-hopping optimization algorithm to find more discriminatory instances
- Starts from the discriminatory instances found in the global search
- Uses gradient-based local perturbation to explore the neighborhood

### Instance Processing
For each input instance, the algorithm:
1. Predicts the outcome using the trained model
2. Creates variants by changing protected attribute values
3. Predicts outcomes for these variants
4. Identifies discriminatory instances where the outcome differs
5. Collects metrics on discrimination by attribute value

## Output

The algorithm returns two main outputs:

### Results DataFrame (res_df)
A pandas DataFrame containing all identified discriminatory pairs with the following columns:
- All original feature columns from the dataset
- `indv_key`: A unique identifier for each individual instance
- `outcome`: The predicted outcome for the instance
- `couple_key`: A key linking two instances that form a discriminatory pair
- `diff_outcome`: The absolute difference in outcomes between the pair
- `case_id`: A unique identifier for each discriminatory case
- `TSN`: Total Sample Number - total number of input pairs tested
- `DSN`: Discriminatory Sample Number - number of discriminatory pairs found
- `SUR`: Success Rate - ratio of DSN to TSN
- `DSS`: Discriminatory Sample Search time - average time to find a discriminatory sample

### Metrics Dictionary (metrics)
A dictionary containing summary statistics:
- `TSN`: Total Sample Number
- `DSN`: Discriminatory Sample Number
- `SUR`: Success Rate (DSN/TSN)
- `DSS`: Average search time per discriminatory sample
- `total_time`: Total execution time
- `dsn_by_attr_value`: Detailed metrics for each protected attribute value

## Example Output DataFrame

Here's an example of what the output DataFrame might look like for the Adult dataset:

| age | workclass | education | education-num | marital-status | occupation | relationship | race | gender | capital-gain | capital-loss | hours-per-week | native-country | indv_key | outcome | couple_key | diff_outcome | case_id | TSN | DSN | SUR | DSS |
|-----|-----------|-----------|---------------|----------------|------------|--------------|------|--------|--------------|--------------|----------------|----------------|----------|---------|------------|--------------|---------|-----|-----|-----|-----|
| 42 | Private | Bachelors | 13 | Married | Exec-mgr | Husband | White | Male | 0 | 0 | 40 | United-States | 42\|1\|9\|13\|2\|4\|0\|4\|1\|0\|0\|40\|39 | 1 | 42\|1\|9\|13\|2\|4\|0\|4\|1\|0\|0\|40\|39-42\|1\|9\|13\|2\|4\|0\|4\|0\|0\|0\|40\|39 | 1 | 0 | 1024 | 157 | 0.153320 | 23.567839 |
| 42 | Private | Bachelors | 13 | Married | Exec-mgr | Husband | White | Female | 0 | 0 | 40 | United-States | 42\|1\|9\|13\|2\|4\|0\|4\|0\|0\|0\|40\|39 | 0 | 42\|1\|9\|13\|2\|4\|0\|4\|1\|0\|0\|40\|39-42\|1\|9\|13\|2\|4\|0\|4\|0\|0\|0\|40\|39 | 1 | 0 | 1024 | 157 | 0.153320 | 23.567839 |
| 35 | Private | HS-grad | 9 | Divorced | Craft-rep | Not-in-family | White | Male | 0 | 0 | 45 | United-States | 35\|1\|7\|9\|5\|6\|4\|4\|1\|0\|0\|45\|39 | 1 | 35\|1\|7\|9\|5\|6\|4\|4\|1\|0\|0\|45\|39-35\|1\|7\|9\|5\|6\|4\|4\|0\|0\|0\|45\|39 | 1 | 1 | 1024 | 157 | 0.153320 | 23.567839 |
| 35 | Private | HS-grad | 9 | Divorced | Craft-rep | Not-in-family | White | Female | 0 | 0 | 45 | United-States | 35\|1\|7\|9\|5\|6\|4\|4\|0\|0\|0\|45\|39 | 0 | 35\|1\|7\|9\|5\|6\|4\|4\|1\|0\|0\|45\|39-35\|1\|7\|9\|5\|6\|4\|4\|0\|0\|0\|45\|39 | 1 | 1 | 1024 | 157 | 0.153320 | 23.567839 |

### Example Interpretation

In this example:
- Each row represents an instance in a discriminatory pair
- Rows with the same `case_id` form a discriminatory pair
- The `diff_outcome` column shows the difference in predictions (1 indicates discrimination)
- The `couple_key` links the two instances in a pair
- The metrics columns (TSN, DSN, SUR, DSS) provide summary statistics

## Use Cases

The algorithm is particularly useful for:
- Identifying specific instances where a model exhibits discriminatory behavior
- Quantifying the extent of discrimination by protected attribute
- Providing concrete examples that can be used to improve model fairness