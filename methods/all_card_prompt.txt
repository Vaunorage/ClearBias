Individual discrimination discovery methods :

### Title : White-box fairness testing through adversarial sampling
### Metric: What metric does the method rely on to find discrimination?

The method, called Adversarial Discrimination Finder (ADF), focuses on finding **individual discrimination**.

*   **Definition**: The paper formally defines an individual discriminatory instance in Definition 2.1 (page 5). An instance `x` is considered an individual discriminatory instance if another instance `x'` exists that meets the following criteria:
    1.  `x'` differs from `x` only in one or more **protected attributes** (e.g., race, gender).
    2.  All **non-protected attributes** remain the same.
    3.  The model's prediction for `x` is different from its prediction for `x'` (i.e., `D(x) ≠ D(x')`).

The method aims to generate these `(x, x')` pairs, which are called "individual discriminatory instance pairs."

### Location: Where does this method try to find discrimination?

The method finds discrimination **within the DNN model** by generating new, adversarial data samples.

*   It is a **white-box** approach, meaning it requires full access to the model's internal structure and parameters.
*   It uses the original dataset as a starting point ("seed instances").
*   It then systematically perturbs these seeds using gradient information from the model to generate new data instances that are not in the original dataset. The goal of these perturbations is to find inputs that cross the model's decision boundary when only a protected attribute is changed, thus revealing discrimination in the model's logic.

### What they find: What exactly does this method try to find and what does it return?

*   **What it finds**: The method's primary goal is to find and generate **individual discriminatory instances**. It does not focus on group-level discrimination (i.e., comparing the outcomes of one entire protected group against another) but on finding specific pairs of individuals who are treated unfairly by the model.

*   **What it returns**: The method returns a **set of individual discriminatory instances**. These are data points (represented as feature vectors) that have been proven to cause a discriminatory outcome from the model. As stated in the algorithm descriptions (page 6 and 8), the functions `return g_id` and `return l_id`, which are sets of these identified instances.

### Performance: How has this method's performance been evaluated and what was the result?

The performance of ADF was evaluated against two state-of-the-art methods, AEQUITAS and SG, across three datasets. The evaluation was based on effectiveness, efficiency, and the usefulness of the generated instances.

*   **Effectiveness (Finding more instances)**:
    *   Compared to AEQUITAS, ADF generated **25 times more** individual discriminatory instances and explored the input space 9 times more effectively.
    *   Compared to SG (given the same time limit), ADF generated **6.5 times more** individual discriminatory instances.
    *   The "success rate" (the ratio of discriminatory instances found to instances generated) of ADF was over twice that of AEQUITAS (40.89% vs 18.22%).

*   **Efficiency (Finding instances faster)**:
    *   ADF was significantly faster than the baselines. The paper reports an "average speedup of **104%** (over 2x faster) compared to AEQUITAS and **588%** (over 6x faster) compared to SG" for generating the same number of discriminatory instances (Answer to RQ2, page 11).

*   **Usefulness (Improving model fairness)**:
    *   The discriminatory instances generated by ADF were used to augment the training data and retrain the model. This process significantly improved the model's fairness.
    *   Retraining with ADF-generated data led to an average fairness improvement of **57.2%**, outperforming the improvements from AEQUITAS (45.1%) and SG (49.1%) (Answer to RQ3, page 12).

# Adversarial Debiasing Framework (ADF) Documentation

## Implementation

The Adversarial Debiasing Framework (ADF) is an algorithm designed to detect and identify discriminatory instances in machine learning models. It focuses on finding pairs of inputs that differ only in their protected attributes but receive different predictions from the model, which indicates potential bias.

## Input

The ADF algorithm takes the following inputs:

### Primary Input
- **data** (DiscriminationData object): Contains the dataset and metadata including:
  - `dataframe`: The complete dataset
  - `training_dataframe`: Data used for training the model
  - `xdf`: Features dataframe used for testing
  - `protected_attributes`: List of column names that are considered protected attributes (e.g., race, gender)
  - `sensitive_indices`: Indices of the protected attributes in the feature array
  - `input_bounds`: Min/max bounds for each feature
  - `outcome_column`: Name of the target variable

### Configuration Parameters
- **max_global** (int, default=2000): Maximum number of samples for the global search phase
- **max_local** (int, default=2000): Maximum number of samples for the local search phase
- **cluster_num** (int, default=10): Number of clusters to divide the input data into
- **random_seed** (int, default=42): Random seed for reproducibility
- **max_runtime_seconds** (int, default=3600): Maximum runtime in seconds before early termination
- **max_tsn** (int, default=None): Maximum number of test samples to generate before termination
- **step_size** (float, default=0.4): Step size for perturbation in both global and local search phases
- **one_attr_at_a_time** (bool, default=False): If True, only one protected attribute is varied at a time
- **db_path** (str, default=None): Path to database for storing results
- **analysis_id** (str, default=None): ID for the current analysis run
- **use_cache** (bool, default=True): Whether to use cached models

## Algorithm Process

The ADF algorithm works in two main phases:

### Global Search Phase
- Clusters the input data to find diverse starting points
- For each starting point, applies global perturbation to find discriminatory instances
- Adds discriminatory instances to a set for the local search phase

### Local Search Phase
- Uses the basin-hopping optimization algorithm to find more discriminatory instances
- Starts from the discriminatory instances found in the global search
- Uses gradient-based local perturbation to explore the neighborhood

### Instance Processing
For each input instance, the algorithm:
1. Predicts the outcome using the trained model
2. Creates variants by changing protected attribute values
3. Predicts outcomes for these variants
4. Identifies discriminatory instances where the outcome differs
5. Collects metrics on discrimination by attribute value

## Output

The algorithm returns two main outputs:

### Results DataFrame (res_df)
A pandas DataFrame containing all identified discriminatory pairs with the following columns:
- All original feature columns from the dataset
- `indv_key`: A unique identifier for each individual instance
- `outcome`: The predicted outcome for the instance
- `couple_key`: A key linking two instances that form a discriminatory pair
- `diff_outcome`: The absolute difference in outcomes between the pair
- `case_id`: A unique identifier for each discriminatory case
- `TSN`: Total Sample Number - total number of input pairs tested
- `DSN`: Discriminatory Sample Number - number of discriminatory pairs found
- `SUR`: Success Rate - ratio of DSN to TSN
- `DSS`: Discriminatory Sample Search time - average time to find a discriminatory sample

### Metrics Dictionary (metrics)
A dictionary containing summary statistics:
- `TSN`: Total Sample Number
- `DSN`: Discriminatory Sample Number
- `SUR`: Success Rate (DSN/TSN)
- `DSS`: Average search time per discriminatory sample
- `total_time`: Total execution time
- `dsn_by_attr_value`: Detailed metrics for each protected attribute value

## Example Output DataFrame

Here's an example of what the output DataFrame might look like for the Adult dataset:

| age | workclass | education | education-num | marital-status | occupation | relationship | race | gender | capital-gain | capital-loss | hours-per-week | native-country | indv_key | outcome | couple_key | diff_outcome | case_id | TSN | DSN | SUR | DSS |
|-----|-----------|-----------|---------------|----------------|------------|--------------|------|--------|--------------|--------------|----------------|----------------|----------|---------|------------|--------------|---------|-----|-----|-----|-----|
| 42 | Private | Bachelors | 13 | Married | Exec-mgr | Husband | White | Male | 0 | 0 | 40 | United-States | 42\|1\|9\|13\|2\|4\|0\|4\|1\|0\|0\|40\|39 | 1 | 42\|1\|9\|13\|2\|4\|0\|4\|1\|0\|0\|40\|39-42\|1\|9\|13\|2\|4\|0\|4\|0\|0\|0\|40\|39 | 1 | 0 | 1024 | 157 | 0.153320 | 23.567839 |
| 42 | Private | Bachelors | 13 | Married | Exec-mgr | Husband | White | Female | 0 | 0 | 40 | United-States | 42\|1\|9\|13\|2\|4\|0\|4\|0\|0\|0\|40\|39 | 0 | 42\|1\|9\|13\|2\|4\|0\|4\|1\|0\|0\|40\|39-42\|1\|9\|13\|2\|4\|0\|4\|0\|0\|0\|40\|39 | 1 | 0 | 1024 | 157 | 0.153320 | 23.567839 |
| 35 | Private | HS-grad | 9 | Divorced | Craft-rep | Not-in-family | White | Male | 0 | 0 | 45 | United-States | 35\|1\|7\|9\|5\|6\|4\|4\|1\|0\|0\|45\|39 | 1 | 35\|1\|7\|9\|5\|6\|4\|4\|1\|0\|0\|45\|39-35\|1\|7\|9\|5\|6\|4\|4\|0\|0\|0\|45\|39 | 1 | 1 | 1024 | 157 | 0.153320 | 23.567839 |
| 35 | Private | HS-grad | 9 | Divorced | Craft-rep | Not-in-family | White | Female | 0 | 0 | 45 | United-States | 35\|1\|7\|9\|5\|6\|4\|4\|0\|0\|0\|45\|39 | 0 | 35\|1\|7\|9\|5\|6\|4\|4\|1\|0\|0\|45\|39-35\|1\|7\|9\|5\|6\|4\|4\|0\|0\|0\|45\|39 | 1 | 1 | 1024 | 157 | 0.153320 | 23.567839 |

### Example Interpretation

In this example:
- Each row represents an instance in a discriminatory pair
- Rows with the same `case_id` form a discriminatory pair
- The `diff_outcome` column shows the difference in predictions (1 indicates discrimination)
- The `couple_key` links the two instances in a pair
- The metrics columns (TSN, DSN, SUR, DSS) provide summary statistics

## Use Cases

The algorithm is particularly useful for:
- Identifying specific instances where a model exhibits discriminatory behavior
- Quantifying the extent of discrimination by protected attribute
- Providing concrete examples that can be used to improve model fairness

### Title: AEQUITAS : Automated Directed Fairness Testing

### Metric: What metric does the method rely on to find discrimination?
The method, AEQUITAS, relies on **individual fairness**. The paper states, "we focus towards individual fairness, as it is critical for eliminating societal bias and aim to check for discrimination that might violate individual fairness [2]."

This is formally defined in **Definition 4.1 (Discriminatory Input and fairness)**, where an input `I` is considered discriminatory if another input `I'`, which is identical to `I` except for its values on protected attributes (e.g., gender), receives a significantly different output from the model (`|f(I) − f(I')| > γ`).

### Location: Where does this method try to find discrimination models or data?
The method finds discrimination in the **machine learning model** itself. It operates as a black-box testing approach. The abstract clearly states, "For a given machine-learning model and a set of sensitive input parameters, our AEQUITAS approach automatically discovers discriminatory inputs that highlight fairness violation." The methodology involves systematically generating inputs and observing the model's outputs to uncover biased behavior, rather than analyzing the training data directly.

### What they find: What exactly does this method try to find and what does the method return?
- **What it finds**: The method aims to find **individual discriminatory inputs**. These are specific input examples that, when compared to a minimally different counterpart (differing only on a sensitive attribute), receive a different classification from the model, thus exposing a fairness violation.

- **What it returns**: The method returns a **set of discriminatory inputs**. The algorithms described in the paper (e.g., `GLOBAL_EXP` and `LOCAL_EXP`) build and return a set (`disc_inps` or `Test`) containing the inputs that were found to cause discriminatory behavior in the model.

### Performance: How did this method's performance been evaluated and what was the result?
The performance of AEQUITAS was evaluated against six classifiers (including SVM, Random Forest, and a "Fair SVM") on the US census income dataset, with "gender" as the sensitive attribute.

The evaluation and results were as follows:

1.  **Effectiveness (Finding Discrimination)**: AEQUITAS significantly outperformed purely random testing. It was more effective by "a factor of 9.6 on average and up to a factor of 20.4". In its most advanced configuration (`fully-directed`), it generated test suites where up to **70% of the inputs were discriminatory**.

2.  **Efficiency (Speed)**: The method was much faster than random testing at finding discriminatory inputs. The `fully-directed` version was **83.27% faster on average** and up to **96.62% faster** in the best case.

3.  **Utility (Improving Fairness)**: The discriminatory inputs generated by AEQUITAS were successfully used to retrain and improve the models. This automated retraining process reduced the percentage of discriminatory inputs by an **average of 43.2%** across the tested models, with a maximum reduction of **94.36%** for the Decision Tree classifier.

Implementation
Aequitas Algorithm: Input and Output Analysis
Input
The Aequitas algorithm is a bias detection method that works by searching for individual discrimination in machine learning models. It takes the following inputs:

DiscriminationData object (
data
): Contains:
training_dataframe: The dataset used to train the model
protected_attributes: List of sensitive attributes (e.g., race, gender, age)
outcome_column: Target variable name
attr_columns: All feature column names
sensitive_indices: Indices of protected attributes in the feature list
input_bounds
: Min and max values for each feature
Algorithm parameters:
model_type: The type of model to train (default: 'rf' for Random Forest)
max_global: Maximum number of global samples to test (default: 500)
max_local: Maximum number of local search iterations (default: 2000)
step_size: Step size for local search (default: 1.0)
max_runtime_seconds: Maximum runtime in seconds (default: 3600)
max_tsn: Maximum number of samples to test (default: 10000)
one_attr_at_a_time: Whether to vary one attribute at a time (default: False)
Algorithm Process
Training: The algorithm first trains a model using the provided training data.
Global Discrimination Discovery: It randomly samples instances from the dataset and tests them for discrimination.
Local Discrimination Discovery: For each discriminatory instance found in the global phase, it performs a local search to find more discriminatory instances in the neighborhood.
Discrimination Testing: For each instance, it creates variants by changing the protected attributes and checks if the model's prediction changes.
Output
The algorithm returns two main outputs:

Results DataFrame (res_df): A pandas DataFrame containing all discovered discriminatory pairs with the following columns:
All feature columns from the original dataset
indv_key: A unique identifier for each individual instance
outcome: The model's prediction for this instance
couple_key: A unique identifier for each pair of instances
diff_outcome: The absolute difference in outcomes between the pair
case_id: A unique identifier for each discrimination case
TSN: Total Sample Number - total number of instances tested
DSN: Discriminatory Sample Number - number of discriminatory instances found
SUR: Success Rate - ratio of DSN to TSN
DSS: Discriminatory Sample Search time - average time to find each discriminatory instance
Metrics Dictionary (
metrics
): Contains:
TSN: Total Sample Number
DSN: Discriminatory Sample Number
SUR: Success Rate (DSN/TSN)
DSS: Discriminatory Sample Search time
total_time: Total execution time
dsn_by_attr_value: Discrimination statistics broken down by protected attribute
Example Output DataFrame
Here's an example of what the output DataFrame might look like for a credit dataset with protected attributes like age and gender:

   age  gender  income  credit_score  loan_amount  outcome  indv_key    couple_key  diff_outcome  case_id   TSN   DSN    SUR     DSS
0   35       0   65000          720        50000        1  35|0|65000|720|50000  35|0|65000|720|50000-35|1|65000|720|50000       1        0  3500   127  0.036  28.346
1   35       1   65000          720        50000        0  35|1|65000|720|50000  35|0|65000|720|50000-35|1|65000|720|50000       1        0  3500   127  0.036  28.346
2   42       0   72000          680        75000        1  42|0|72000|680|75000  42|0|72000|680|75000-42|1|72000|680|75000       1        1  3500   127  0.036  28.346
3   42       1   72000          680        75000        0  42|1|72000|680|75000  42|0|72000|680|75000-42|1|72000|680|75000       1        1  3500   127  0.036  28.346
4   28       0   45000          650        30000        0  28|0|45000|650|30000  28|0|45000|650|30000-28|1|45000|650|30000       1        2  3500   127  0.036  28.346
5   28       1   45000          650        30000        1  28|1|45000|650|30000  28|0|45000|650|30000-28|1|45000|650|30000       1        2  3500   127  0.036  28.346
In this example:

Each pair of rows represents a discriminatory pair where changing only the gender attribute (from 0 to 1 or vice versa) causes the model's prediction to change from 0 to 1 or 1 to 0.
The diff_outcome column shows the absolute difference in predictions (always 1 for binary classification).
The metrics columns (TSN, DSN, SUR, DSS) are the same for all rows, representing the overall algorithm performance.
The algorithm found 127 discriminatory instances out of 3500 tested samples (SUR = 0.036), with an average time of 28.346 seconds to find each discriminatory instance.

### Title: AEQUITAS : Automated Directed Fairness Testing

### Metric: What metric does the method rely on to find discrimination?
The method, AEQUITAS, relies on **individual fairness**. The paper states, "we focus towards individual fairness, as it is critical for eliminating societal bias and aim to check for discrimination that might violate individual fairness [2]."

This is formally defined in **Definition 4.1 (Discriminatory Input and fairness)**, where an input `I` is considered discriminatory if another input `I'`, which is identical to `I` except for its values on protected attributes (e.g., gender), receives a significantly different output from the model (`|f(I) − f(I')| > γ`).

### Location: Where does this method try to find discrimination models or data?
The method finds discrimination in the **machine learning model** itself. It operates as a black-box testing approach. The abstract clearly states, "For a given machine-learning model and a set of sensitive input parameters, our AEQUITAS approach automatically discovers discriminatory inputs that highlight fairness violation." The methodology involves systematically generating inputs and observing the model's outputs to uncover biased behavior, rather than analyzing the training data directly.

### What they find: What exactly does this method try to find and what does the method return?
- **What it finds**: The method aims to find **individual discriminatory inputs**. These are specific input examples that, when compared to a minimally different counterpart (differing only on a sensitive attribute), receive a different classification from the model, thus exposing a fairness violation.

- **What it returns**: The method returns a **set of discriminatory inputs**. The algorithms described in the paper (e.g., `GLOBAL_EXP` and `LOCAL_EXP`) build and return a set (`disc_inps` or `Test`) containing the inputs that were found to cause discriminatory behavior in the model.

### Performance: How did this method's performance been evaluated and what was the result?
The performance of AEQUITAS was evaluated against six classifiers (including SVM, Random Forest, and a "Fair SVM") on the US census income dataset, with "gender" as the sensitive attribute.

The evaluation and results were as follows:

1.  **Effectiveness (Finding Discrimination)**: AEQUITAS significantly outperformed purely random testing. It was more effective by "a factor of 9.6 on average and up to a factor of 20.4". In its most advanced configuration (`fully-directed`), it generated test suites where up to **70% of the inputs were discriminatory**.

2.  **Efficiency (Speed)**: The method was much faster than random testing at finding discriminatory inputs. The `fully-directed` version was **83.27% faster on average** and up to **96.62% faster** in the best case.

3.  **Utility (Improving Fairness)**: The discriminatory inputs generated by AEQUITAS were successfully used to retrain and improve the models. This automated retraining process reduced the percentage of discriminatory inputs by an **average of 43.2%** across the tested models, with a maximum reduction of **94.36%** for the Decision Tree classifier.

Implementation
Aequitas Algorithm: Input and Output Analysis
Input
The Aequitas algorithm is a bias detection method that works by searching for individual discrimination in machine learning models. It takes the following inputs:

DiscriminationData object (
data
): Contains:
training_dataframe: The dataset used to train the model
protected_attributes: List of sensitive attributes (e.g., race, gender, age)
outcome_column: Target variable name
attr_columns: All feature column names
sensitive_indices: Indices of protected attributes in the feature list
input_bounds
: Min and max values for each feature
Algorithm parameters:
model_type: The type of model to train (default: 'rf' for Random Forest)
max_global: Maximum number of global samples to test (default: 500)
max_local: Maximum number of local search iterations (default: 2000)
step_size: Step size for local search (default: 1.0)
max_runtime_seconds: Maximum runtime in seconds (default: 3600)
max_tsn: Maximum number of samples to test (default: 10000)
one_attr_at_a_time: Whether to vary one attribute at a time (default: False)
Algorithm Process
Training: The algorithm first trains a model using the provided training data.
Global Discrimination Discovery: It randomly samples instances from the dataset and tests them for discrimination.
Local Discrimination Discovery: For each discriminatory instance found in the global phase, it performs a local search to find more discriminatory instances in the neighborhood.
Discrimination Testing: For each instance, it creates variants by changing the protected attributes and checks if the model's prediction changes.
Output
The algorithm returns two main outputs:

Results DataFrame (res_df): A pandas DataFrame containing all discovered discriminatory pairs with the following columns:
All feature columns from the original dataset
indv_key: A unique identifier for each individual instance
outcome: The model's prediction for this instance
couple_key: A unique identifier for each pair of instances
diff_outcome: The absolute difference in outcomes between the pair
case_id: A unique identifier for each discrimination case
TSN: Total Sample Number - total number of instances tested
DSN: Discriminatory Sample Number - number of discriminatory instances found
SUR: Success Rate - ratio of DSN to TSN
DSS: Discriminatory Sample Search time - average time to find each discriminatory instance
Metrics Dictionary (
metrics
): Contains:
TSN: Total Sample Number
DSN: Discriminatory Sample Number
SUR: Success Rate (DSN/TSN)
DSS: Discriminatory Sample Search time
total_time: Total execution time
dsn_by_attr_value: Discrimination statistics broken down by protected attribute
Example Output DataFrame
Here's an example of what the output DataFrame might look like for a credit dataset with protected attributes like age and gender:

   age  gender  income  credit_score  loan_amount  outcome  indv_key    couple_key  diff_outcome  case_id   TSN   DSN    SUR     DSS
0   35       0   65000          720        50000        1  35|0|65000|720|50000  35|0|65000|720|50000-35|1|65000|720|50000       1        0  3500   127  0.036  28.346
1   35       1   65000          720        50000        0  35|1|65000|720|50000  35|0|65000|720|50000-35|1|65000|720|50000       1        0  3500   127  0.036  28.346
2   42       0   72000          680        75000        1  42|0|72000|680|75000  42|0|72000|680|75000-42|1|72000|680|75000       1        1  3500   127  0.036  28.346
3   42       1   72000          680        75000        0  42|1|72000|680|75000  42|0|72000|680|75000-42|1|72000|680|75000       1        1  3500   127  0.036  28.346
4   28       0   45000          650        30000        0  28|0|45000|650|30000  28|0|45000|650|30000-28|1|45000|650|30000       1        2  3500   127  0.036  28.346
5   28       1   45000          650        30000        1  28|1|45000|650|30000  28|0|45000|650|30000-28|1|45000|650|30000       1        2  3500   127  0.036  28.346
In this example:

Each pair of rows represents a discriminatory pair where changing only the gender attribute (from 0 to 1 or vice versa) causes the model's prediction to change from 0 to 1 or 1 to 0.
The diff_outcome column shows the absolute difference in predictions (always 1 for binary classification).
The metrics columns (TSN, DSN, SUR, DSS) are the same for all rows, representing the overall algorithm performance.
The algorithm found 127 discriminatory instances out of 3500 tested samples (SUR = 0.036), with an average time of 28.346 seconds to find each discriminatory instance.

### **Title**
FlipTest: Fairness Testing via Optimal Transport

### **Metric**
The method relies on **Optimal Transport** to find a mapping between two protected groups (e.g., men and women). This mapping pairs each individual in the source group with a "similar" counterpart in the target group, minimizing the overall "distance" or cost between all pairs.

The core metric is the **flipset**, defined as "the set of individuals whose classifier output changes post-translation" (Abstract). This set is further divided into:
*   **Positive flipset (F+):** Individuals who are advantaged by their group membership (e.g., a woman is hired, but her male counterpart is not).
*   **Negative flipset (F-):** Individuals who are disadvantaged by their group membership (e.g., a woman is rejected, but her male counterpart is hired).

Discrimination is assessed by analyzing the size, balance, and composition of these flipsets.

### **Individual discrimination, group, subgroup discrimination (granularity and intersectionality)**
FlipTest operates on multiple levels of granularity:
*   **Individual Discrimination:** The method creates pairs of "similar" individuals from different groups. A change in the model's outcome for a specific pair is considered evidence of potential individual-level discrimination.
*   **Group Discrimination:** By comparing the relative sizes of the positive and negative flipsets, the method can test for group-level fairness criteria like demographic parity. For example, a much larger negative flipset than a positive one suggests a group-level bias.
*   **Subgroup Discrimination:** The method can uncover discrimination even when group-level metrics are satisfied. It does this by analyzing the feature distributions of the individuals *within* the flipset and comparing them to the overall population. The paper states, "By comparing the distribution of the flipsets to the distribution of the overall population, it is often possible to identify specific subgroups that the model discriminates against" (Section 2).

### **Location**
The method finds discrimination in the **model**. It is a black-box testing technique that queries a trained classifier to observe its behavior on specifically crafted inputs. The goal is to "uncover[] discrimination in classifiers" (Abstract) by analyzing the model's output on real (in-distribution) samples and their generated counterparts.

### **What they find**
FlipTest aims to find **salient patterns of discriminatory behavior** in a model. It does not claim to prove a causal link between a protected attribute and the outcome. Instead, it identifies:
1.  **Potentially Discriminated Individuals:** The members of the flipset are concrete examples of individuals who may be harmed or advantaged by the model due to their group membership.
2.  **Discriminated Subgroups:** It identifies which subgroups are most affected by analyzing the characteristics of the individuals in the flipset (e.g., finding that the model harms "shorter-haired women" as in Section 2).
3.  **Associated Features:** It identifies which features are most associated with the discriminatory behavior, providing insight into *how* the model might be discriminating.

### **What does the method return in terms of data structure?**
The method returns two main outputs:
1.  **The Flipset:** A set of individuals from the source population whose model-predicted label changes when they are mapped to their counterparts in the target population. This is partitioned into a positive flipset and a negative flipset.
2.  **A Transparency Report:** A ranked list of features that are most associated with the model's differing behavior on the flipset. This report shows (1) the average change for each feature between an individual and their counterpart and (2) how consistently that feature changes in a specific direction (e.g., always increasing). This helps auditors understand the potential mechanism of discrimination.

### **Performance**
The performance of FlipTest was evaluated empirically across four datasets, including real-world case studies (predictive policing and hiring) and comparisons with other fairness auditing methods.

*   **Case Studies:**
    *   On a predictive policing dataset (SSL), FlipTest identified a model's bias against younger black individuals with more narcotics arrests, and the transparency report correctly highlighted "narcotics arrests" as the key feature driving the bias (Section 5.2).
    *   On a synthetic hiring dataset, it successfully detected subgroup discrimination (harming short-haired women) in a model that was designed to be fair at the group level (Section 5.3).
*   **Comparison to Other Methods:**
    *   **vs. Counterfactual Fairness:** On a law school dataset, FlipTest produced "nearly identical results" to the counterfactual fairness method without requiring access to a causal model, which is a major practical advantage (Section 5.4).
    *   **vs. FairTest:** On a synthetic dataset, FlipTest was shown to identify discrimination based on features that are themselves biased (i.e., have different distributions across groups), a scenario that FairTest is not well-suited for (Section 5.5).

# FlipTest Algorithm Implementation

## Overview

The FlipTest algorithm is implemented in the `main.py` file and provides two main functions for analyzing discrimination in datasets:

- `run_fliptest_on_dataset` - Runs FlipTest on a single protected attribute
- `run_fliptest` - Runs FlipTest on multiple protected attributes in a dataset

## Functions

### `run_fliptest_on_dataset`

This function analyzes discrimination for a specific protected attribute.

#### Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `discrimination_data` | DiscriminationData | Required | Dataset object containing the dataframe with features, outcomes, and protected attributes |
| `protected_attribute` | str | Required | Name of the protected attribute column to analyze (e.g., "race", "gender") |
| `group1_val` | Any | 0 | Value representing the first group in the protected attribute column |
| `group2_val` | Any | 1 | Value representing the second group in the protected attribute column |

### `run_fliptest`

This function runs FlipTest on multiple protected attributes.

#### Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `data_obj` | DiscriminationData | Required | Dataset object containing the dataframe with features, outcomes, and protected attributes |
| `max_runs` | int | None | Maximum number of protected attributes to check. If None, all protected attributes will be checked |

## Algorithm Process

FlipTest works by:

1. Splitting the dataset into two groups based on the protected attribute
2. Scaling the features to normalize them
3. Computing a distance matrix between individuals in different groups
4. Using optimal transport to find the closest matching pairs between groups
5. Identifying pairs with different outcomes as potentially discriminatory

## Output

The algorithm returns a tuple containing two elements:

### 1. Results DataFrame (`results_df`)

A pandas DataFrame containing all identified discriminatory pairs with the following columns:

- **All original feature columns** from the dataset
- **`indv_key`**: A unique identifier for each individual instance
- **`outcome`**: The predicted outcome for the instance
- **`couple_key`**: A key linking two instances that form a discriminatory pair
- **`diff_outcome`**: The absolute difference in outcomes between the pair
- **`case_id`**: A unique identifier for each discriminatory case
- **`TSN`**: Total Sample Number - total number of input pairs tested
- **`DSN`**: Discriminatory Sample Number - number of discriminatory pairs found
- **`SUR`**: Success Rate - ratio of DSN to TSN
- **`DSS`**: Discriminatory Sample Search time - average time to find a discriminatory sample
- **`protected_attribute`**: The protected attribute used for this pair

### 2. Metrics Dictionary (`metrics_dict`)

A dictionary containing summary statistics:

- **`total_runs`**: Total number of protected attributes tested
- **`successful_runs`**: Number of protected attributes that completed successfully
- **`total_time`**: Total execution time
- **`attribute_metrics`**: Detailed metrics for each protected attribute, including:
  - `TSN`: Total Sample Number
  - `DSN`: Discriminatory Sample Number
  - `SUR`: Success Rate
  - `DSS`: Discriminatory Sample Search time
  - `total_time`: Execution time for this attribute
  - `mean_distance`: Mean L1 distance between matched pairs
  - `protected_attribute`: Name of the protected attribute
  - `group1_val`: Value for group 1
  - `group2_val`: Value for group 2
  - `raw_results`: Raw data used in the analysis

## Example Usage

### Input Example

```python
from data_generator.main import DiscriminationData, generate_optimal_discrimination_data

# Generate synthetic data with controlled bias
data_obj = generate_optimal_discrimination_data(
    nb_groups=100,
    nb_attributes=15,
    prop_protected_attr=0.3,
    nb_categories_outcome=1,
    use_cache=True
)

# Run FlipTest on the top 2 most balanced protected attributes
results_df, metrics = run_fliptest(data_obj, max_runs=2)
```

### Output Example

#### Results DataFrame Example

```
   attr1  attr2  attr3  gender  income  ...  couple_key  diff_outcome  case_id  TSN  DSN   SUR    DSS  protected_attribute
0    0.3    0.7    1.2       0       0  ...  0.3|0.7|...-0.4|0.6|...          1    1     87   43  0.494  0.023  gender
1    0.4    0.6    1.1       1       1  ...  0.3|0.7|...-0.4|0.6|...          1    1     87   43  0.494  0.023  gender
2    0.5    0.2    0.9       0       0  ...  0.5|0.2|...-0.6|0.3|...          2    2     87   43  0.494  0.023  gender
3    0.6    0.3    0.8       1       1  ...  0.5|0.2|...-0.6|0.3|...          2    2     87   43  0.494  0.023  gender
4    0.1    0.8    1.5       0       0  ...  0.1|0.8|...-0.2|0.9|...          3    3     87   43  0.494  0.023  gender
```

#### Metrics Dictionary Example

```python
{
    "total_runs": 2,
    "successful_runs": 2,
    "total_time": 5.23,
    "attribute_metrics": {
        "gender": {
            "TSN": 87,
            "DSN": 43,
            "SUR": 0.494,
            "DSS": 0.023,
            "total_time": 2.12,
            "mean_distance": 0.876,
            "protected_attribute": "gender",
            "group1_val": 0,
            "group2_val": 1,
            "raw_results": {
                # Raw data used in the analysis
            }
        },
        "race": {
            "TSN": 92,
            "DSN": 38,
            "SUR": 0.413,
            "DSS": 0.031,
            "total_time": 3.11,
            "mean_distance": 0.912,
            "protected_attribute": "race",
            "group1_val": 0,
            "group2_val": 1,
            "raw_results": {
                # Raw data used in the analysis
            }
        }
    }
}
```

## Use Case

The FlipTest algorithm is particularly useful for identifying individual fairness violations by finding similar individuals from different protected groups who received different outcomes, suggesting potential discrimination in the decision-making process.

### **Title**
An efficient discrimination discovery method for fairness testing.

### **Metric**
The method relies on **individual discrimination**.

*   **Definition:** Individual discrimination is defined as occurring when a machine learning model gives different results to two similar individuals who only differ in their protected attributes (e.g., gender, race).
*   **Formalization (from Definition 1):** A data item `I` is considered discriminatory if there exists a counterpart `I'` that differs from `I` only in a set of protected attributes `Q`, and the difference in the model's output `|f(I) - f(I')|` is greater than a pre-determined threshold `γ`.

### **Location**
The method finds discrimination in **machine learning models (classifiers)**. It is a black-box testing approach, meaning it does not need access to the model's internal structure but queries it with different inputs to observe its behavior.

### **What they find**
The method aims to find and report **individual discriminatory data items**. These are specific input data instances (test cases) that trigger unfair behavior from the model, as defined by the individual discrimination metric. It does not find discriminated groups but rather individual instances of discrimination.

### **What does the method return in terms of data structure?**
The method returns a **set of discriminatory data items**. As described in Algorithms 1 and 3, the output is a collection (like a list or set, denoted as `D` or `D_local`) of data items that have been evaluated and confirmed to be discriminatory.

### **Performance**
The performance of KOSEI was evaluated by comparing it against a landmark technique, AEQUITAS, on which it is based. The evaluation used three datasets, three types of classifiers, and was measured on detection ability and efficiency.

*   **Discrimination Detection:** KOSEI significantly outperformed AEQUITAS, detecting **5,084.8% more discriminatory instances** on average across all experimental configurations.
*   **Execution Time:** KOSEI was much more efficient, running on average in just **7.5% of the execution time** of AEQUITAS (or 13.3 times faster).
*   **Test Case Quality:**
    *   **Precision:** KOSEI's generated test cases were **3.04 times more likely** to be discriminatory (higher precision) than those generated by AEQUITAS.
    *   **Duplication:** KOSEI is designed to avoid evaluating duplicated test cases, whereas the evaluation showed that in AEQUITAS, **98.8% of evaluated data were duplicates**, leading to inefficiency.

# KOSEI Algorithm Documentation

## Implementation

The KOSEI (Knowledge-guided Optimization-based Search for Effective Instances) algorithm is an individual fairness testing method designed to identify discriminatory instances in machine learning models. Here's a detailed breakdown of its input parameters and output.

## Input Parameters

### Main Parameters

**`model`**: `Optional[Union[Callable[[np.ndarray], Any], str]] = None`
- A callable model that takes a numpy array as input and returns a prediction
- Can also be a string specifying the model type to train (e.g., 'rf' for Random Forest)
- If None, a model will be trained using the provided discrimination_data

**`discrimination_data`**: `Optional[DiscriminationData] = None`
- A DiscriminationData object containing dataset information
- Includes training data, attribute columns, sensitive attributes, and input bounds
- If provided, other dataset-related parameters are extracted from this object

**`protected_attribute_indices`**: `Optional[List[int]] = None`
- List of indices for protected attributes (used if discrimination_data is None)
- Example: `[0, 1]` indicates the first two attributes are protected

**`attribute_domains`**: `Optional[Dict[int, tuple]] = None`
- Dictionary mapping attribute indices to their domains (min, max)
- Example: `{0: (0, 1), 1: (0, 5)}` means attribute 0 has domain [0,1] and attribute 1 has domain [0,5]

**`total_attributes`**: `Optional[int] = None`
- Total number of attributes (used if discrimination_data is None)
- Example: 10 for a dataset with 10 features

**`gamma`**: `float = 0.0`
- Threshold for determining discrimination
- Default is 0.0, meaning any difference in prediction is considered discriminatory

**`model_type`**: `str = 'rf'`
- Type of model to train if no model is provided
- Default is 'rf' (Random Forest)
- Other options could include 'lr' (Logistic Regression), 'svm', etc.

**`model_params`**: `Optional[Dict] = None`
- Parameters for the model to train if no model is provided
- Example: `{'n_estimators': 100, 'max_depth': 5}` for Random Forest

### Search Parameters

**`num_samples`**: `int = 1000`
- Number of random samples to generate in the global search phase
- Higher values increase the chance of finding discriminatory instances but take longer

**`local_search_limit`**: `int = 500`
- Maximum number of iterations for the local search phase
- Controls how thoroughly the algorithm explores around initial seeds

**`random_seed`**: `int = 42`
- Seed for random number generation to ensure reproducibility
- Default is 42

**`max_runtime_seconds`**: `int = 3600`
- Maximum runtime in seconds (default: 1 hour)
- Algorithm will terminate early if this time limit is reached

## Output

The algorithm returns a tuple containing two elements:

**`results_df`**: `pd.DataFrame`
- A DataFrame containing all discriminatory pairs found
- Each row represents an individual instance that is part of a discriminatory pair

**`metrics`**: `Dict`
- A dictionary with summary metrics about the discrimination found

### Example Output DataFrame

The DataFrame has the following structure:

```python
# Example results_df structure
"""
   attr_1  attr_2  ...  attr_n  indv_key  outcome  couple_key  diff_outcome  case_id  TSN  DSN  SUR  DSS
0     0.0     1.0  ...     2.0    0|1|2        1  0|1|2-0|0|2            1        0  500   45  0.09  2.5
1     0.0     0.0  ...     2.0    0|0|2        0  0|1|2-0|0|2            1        0  500   45  0.09  2.5
2     1.0     0.0  ...     1.0    1|0|1        1  1|0|1-1|1|1            1        1  500   45  0.09  2.5
3     1.0     1.0  ...     1.0    1|1|1        0  1|0|1-1|1|1            1        1  500   45  0.09  2.5
...    ...     ...  ...     ...       ...      ...         ...          ...      ...  ...  ...  ...  ...
"""
```

Where:
- **`attr_1, attr_2, ..., attr_n`**: The attribute values for each instance
- **`indv_key`**: A string representation of the instance (concatenated attribute values)
- **`outcome`**: The model's prediction for this instance
- **`couple_key`**: A key linking the two instances in a discriminatory pair
- **`diff_outcome`**: The absolute difference in outcomes between the paired instances
- **`case_id`**: An identifier for each discriminatory pair
- **`TSN`**: Total Sample Number - total number of samples tested
- **`DSN`**: Discriminatory Sample Number - total number of discriminatory pairs found
- **`SUR`**: Success Rate (DSN/TSN) - ratio of discriminatory pairs to total samples
- **`DSS`**: Discriminatory Sample Search time - average time to find a discriminatory pair

For each discriminatory pair, there are two rows in the DataFrame (the original instance and the perturbed instance).

### Example Metrics Dictionary

```python
# Example metrics structure
metrics = {
    'TSN': 1500,                # Total Sample Number
    'DSN': 45,                  # Discriminatory Sample Number
    'SUR': 0.03,                # Success Rate (3% of tested inputs showed discrimination)
    'DSS': 2.5,                 # Average search time per discriminatory sample (seconds)
    'total_time': 112.5,        # Total runtime in seconds
    'dsn_by_attr_value': {
        'total': {'TSN': 1500, 'DSN': 45},
        'attr_0_0': {'TSN': 200, 'DSN': 15, 'SUR': 0.075, 'DSS': 2.5},  # Attribute 0 with value 0
        'attr_0_1': {'TSN': 300, 'DSN': 30, 'SUR': 0.10, 'DSS': 2.5},   # Attribute 0 with value 1
        # ... other attribute-value combinations
    }
}
```

Where:
- **`TSN`**: Total Sample Number - number of samples tested
- **`DSN`**: Discriminatory Sample Number - number of discriminatory pairs found
- **`SUR`**: Success Rate (DSN/TSN) - ratio of discriminatory pairs to total samples
- **`DSS`**: Discriminatory Sample Search time - average time to find a discriminatory pair
- **`total_time`**: Total runtime in seconds
- **`dsn_by_attr_value`**: Breakdown of discrimination by attribute values

## Example Usage

```python
from data_generator.main import get_real_data
from methods.individual.kosei.main import run_kosei

# Load a dataset (e.g., Adult Income dataset)
discrimination_data, data_schema = get_real_data('adult', use_cache=True)

# Run KOSEI with default parameters
results_df, metrics = run_kosei(data=discrimination_data, num_samples=100, local_search_limit=50)

# Print metrics summary
print(f"Testing Metrics: {metrics}")

# Analyze discriminatory pairs
print(f"Found {metrics['DSN']} discriminatory pairs")
print(results_df.head())

# Get unique discriminatory pairs
unique_pairs = results_df['case_id'].unique()
print(f"Number of unique discriminatory pairs: {len(unique_pairs)}")
```

## Algorithm Overview

The KOSEI algorithm works in two phases:

1. **Global Search**: Randomly samples instances to find initial discriminatory seeds
2. **Local Search**: Explores around these seeds to find more discriminatory instances

This approach efficiently identifies individual fairness violations in machine learning models, helping detect bias against protected attributes.

### **Title**
Latent Imitator: Generating Natural Individual Discriminatory Instances for Black-Box Fairness Testing

### **Metric**
The method does not rely on a standard explainability metric (like SHAP or LIME). Instead, its core mechanism is based on the **geometric properties of a Generative Adversarial Network's (GAN) latent space**.

1.  **Surrogate Decision Boundary**: The method first trains a GAN on the dataset. It then generates a large number of synthetic data points and gets their prediction labels from the target black-box model. Using these latent vectors and their corresponding model predictions, it trains a simple linear classifier (an SVM) in the latent space. This classifier acts as a **surrogate decision boundary**.

2.  **Proximity to Boundary**: The primary "metric" for finding discrimination is the **proximity of a latent vector to this surrogate boundary**. The hypothesis is that instances near the decision boundary are most likely to have their prediction flipped with a small perturbation.

3.  **Probing**: For a given latent vector, the method calculates its projection directly onto the surrogate boundary and also probes two nearby points on either side. These three latent vectors are then converted into data instances and tested for discrimination.

### **Individual discrimination, group, subgroup discrimination (granularity and intersectionality)**
-   **Primary Focus (Granularity)**: The method is explicitly designed for **individual fairness testing**. It aims to find specific pairs of instances `(x, x')` that are identical except for a protected attribute but receive different model predictions. The paper argues this is a more granular and powerful approach than group fairness, as it can detect discrimination that group metrics might miss.

-   **Intersectionality (Subgroup)**: While the main experiments focus on a single protected attribute at a time, the paper demonstrates in Section 6 and Table 7 that the framework can be extended to handle **intersectional discrimination**. It evaluates its performance when testing for discrimination across combinations of multiple protected attributes simultaneously (e.g., `gender & race`, `gender & age`).

### **Location**
The method attempts to find discrimination in the **target machine learning model's behavior**.

The search for discriminatory instances does not happen in the original data space directly. Instead, it operates primarily in the **semantic latent space of a pre-trained Generative Adversarial Network (GAN)**. By approximating the model's decision boundary within this latent space, it identifies promising candidate regions. The final discriminatory instances are then generated from these latent vectors for testing against the model.

### **What they find**
The method finds and generates **natural individual discriminatory instances**.

Specifically, it aims to find a data instance `x` (e.g., a loan application) such that if its protected attribute is changed to create a new instance `x'` (e.g., changing gender from 'female' to 'male'), the model's prediction changes (`f(x) != f(x')`).

A key contribution is the **naturalness** of these instances. Because they are generated by a GAN trained on the real data distribution, they are claimed to be more realistic and less likely to violate real-world constraints compared to instances generated by other methods.

### **What does the method return in terms of data structure?**
The method returns a **set of individual discriminatory instances (`D_idi`)**. Each element in this set is a single data record (e.g., a row from a table or an image) that has been confirmed to cause a discriminatory outcome. That is, for each instance in the output set, flipping its protected attribute value results in a different prediction from the model.

### **Performance**
The method's performance was evaluated against 6 other state-of-the-art fairness testing methods on four tabular datasets and three model types (DNN, RF, SVM). The evaluation was based on effectiveness, efficiency, naturalness, and utility.

-   **Evaluation Metrics**:
    -   **Effectiveness**: `#D_idi` (total number of discriminatory instances found in a fixed time).
    -   **Efficiency**: `EGS` (number of discriminatory instances found per second).
    -   **Naturalness**: `ATN` (Average Tabular Naturalness), a metric measuring the statistical similarity between the generated data and the original data distribution.
    -   **Utility**: Improvement in fairness metrics (`IF_r`, `IF_o`, `SPD`, `AOD`) after retraining the model on the instances generated by the method.

-   **Results**:
    -   **Effectiveness & Efficiency**: LIMI significantly outperformed other baselines, generating on average **9.42 times more** discriminatory instances at a speed **8.71 times faster**.
    -   **Naturalness**: The instances generated by LIMI were quantifiably more natural, showing an average **19.65% improvement** in the ATN score compared to other methods.
    -   **Utility for Fairness Improvement**: Retraining a model with the data generated by LIMI led to substantial fairness improvements:
        -   **45.67%** on Individual Fairness (IF_r)
        -   **32.81%** on Individual Fairness (IF_o)
        -   **9.86%** on Group Fairness (SPD)
        -   **28.38%** on Group Fairness (AOD)

# LIMI (Latent Imitator) Algorithm Documentation

## Implementation

The LIMI (Latent Imitator) algorithm is designed to find discriminatory instances in machine learning models by exploring the latent space.

## Input Parameters

### discrimination_data: DiscriminationData
A structured object containing:
- **training_dataframe**: The dataset used for training
- **outcome_column**: Target variable column name
- **protected_attributes**: List of sensitive attributes (e.g., race, gender)
- **sensitive_indices**: Indices of protected attributes in the feature array
- **feature_names**: Names of all features in the dataset

### lambda_val (default: 0.3)
- Controls the step size for probing around the surrogate decision boundary
- Higher values create larger steps in the latent space when searching for discriminatory instances

### n_test_samples (default: 2000)
- Number of random latent vectors generated for testing
- Increasing this value improves the chance of finding discriminatory instances

### n_approx_samples (default: 50000)
- Number of samples used to approximate the decision boundary in latent space
- Higher values lead to more accurate boundary approximation

## Algorithm Process

### 1. Black-box Model Training
- Trains a RandomForest classifier on the provided data
- This model will be tested for fairness

### 2. Generator Creation
- Uses PCA to create a "generator" that simulates a GAN's latent space
- The latent dimension is set to `min(10, n_features-1)`

### 3. Latent Boundary Approximation
- Generates synthetic samples in latent space
- Filters for high-confidence samples (confidence > 0.7)
- Balances the dataset using RandomOverSampler
- Trains a linear SVM as a surrogate model

### 4. Candidate Probing
- Finds candidate points near the surrogate boundary
- Uses the `lambda_val` parameter to determine step size

### 5. Verification and Generation
- Generates data from latent candidates
- Swaps protected attribute values to check for discrimination
- Records instances where predictions change after swapping

## Output

The algorithm returns two elements:

1. **results_df**: A pandas DataFrame containing discriminatory instances
2. **metrics**: A dictionary with testing metrics

## Results DataFrame Structure

The `results_df` contains pairs of instances where changing protected attributes led to different predictions:

- All original feature columns from the input data
- **indv_key**: A string representation of the instance's feature values
- **outcome**: The model's prediction for this instance (0 or 1)
- **couple_key**: Identifier linking original and modified instances in a discriminatory pair
- **diff_outcome**: Absolute difference in predictions (always 1 for discriminatory pairs)
- **case_id**: Unique identifier for each discriminatory case
- **TSN**: Total Sample Number - total instances tested
- **DSN**: Discriminatory Sample Number - number of discriminatory pairs found
- **SUR**: Success Rate - ratio of DSN to TSN
- **DSS**: Discriminatory Sample Search time - average time to find a discriminatory instance

## Metrics Dictionary

The metrics dictionary contains:

- **TSN**: Total number of samples tested
- **DSN**: Number of discriminatory samples found
- **SUR**: Success Rate (DSN/TSN)
- **DSS**: Average time to find a discriminatory instance
- **total_time**: Total execution time
- **dsn_by_attr_value**: Detailed metrics broken down by protected attribute values

## Example Output

When running the algorithm on the Adult dataset, the output might look like:

### Example results_df (first few rows):
"",Attr1_X,Attr2_X,Attr3_X,Attr4_X,Attr5_X,Attr6_X,Attr7_X,Attr8_T,Attr9_T,Attr10_X,Attr11_X,Attr12_X,Attr13_X,indv_key,outcome,case_id,couple_key,diff_outcome,TSN,DSN,SUR,DSS
0,36.730020874327124,1.7674253632127601,10.910858179709088,8.728824403214409,3.8277039091840477,6.076156220686422,1.176057039831307,3.6331849683556348,0.734409876401179,95.65871681559253,93.94753381227623,57.478569630721275,39.17596515064098,36.730020874327124|1.7674253632127601|10.910858179709088|8.728824403214409|3.8277039091840477|6.076156220686422|1.176057039831307|3.6331849683556348|0.734409876401179|95.65871681559253|93.94753381227623|57.478569630721275|39.17596515064098,1,0,36.730020874327124|1.7674253632127601|10.910858179709088|8.728824403214409|3.8277039091840477|6.076156220686422|1.176057039831307|3.6331849683556348|0.734409876401179|95.65871681559253|93.94753381227623|57.478569630721275|39.17596515064098-36.730020874327124|1.7674253632127601|10.910858179709088|8.728824403214409|3.8277039091840477|6.076156220686422|1.176057039831307|1.0|4.0|95.65871681559253|93.94753381227623|57.478569630721275|39.17596515064098,1,200,100,0.5,2.7970831990242004
1,36.730020874327124,1.7674253632127601,10.910858179709088,8.728824403214409,3.8277039091840477,6.076156220686422,1.176057039831307,1.0,4.0,95.65871681559253,93.94753381227623,57.478569630721275,39.17596515064098,36.730020874327124|1.7674253632127601|10.910858179709088|8.728824403214409|3.8277039091840477|6.076156220686422|1.176057039831307|1.0|4.0|95.65871681559253|93.94753381227623|57.478569630721275|39.17596515064098,0,0,36.730020874327124|1.7674253632127601|10.910858179709088|8.728824403214409|3.8277039091840477|6.076156220686422|1.176057039831307|3.6331849683556348|0.734409876401179|95.65871681559253|93.94753381227623|57.478569630721275|39.17596515064098-36.730020874327124|1.7674253632127601|10.910858179709088|8.728824403214409|3.8277039091840477|6.076156220686422|1.176057039831307|1.0|4.0|95.65871681559253|93.94753381227623|57.478569630721275|39.17596515064098,1,200,100,0.5,2.7970831990242004
2,37.27352858595542,3.118703571575923,11.117755224707022,8.360246283448527,4.2417508527101235,5.999518399195046,1.7769820957582605,3.5601620014780373,0.6389484515148515,95.58556957364911,93.93695666508452,57.4938102109297,35.88318863063039,37.27352858595542|3.118703571575923|11.117755224707022|8.360246283448527|4.2417508527101235|5.999518399195046|1.7769820957582605|3.5601620014780373|0.6389484515148515|95.58556957364911|93.93695666508452|57.4938102109297|35.88318863063039,1,1,37.27352858595542|3.118703571575923|11.117755224707022|8.360246283448527|4.2417508527101235|5.999518399195046|1.7769820957582605|3.5601620014780373|0.6389484515148515|95.58556957364911|93.93695666508452|57.4938102109297|35.88318863063039-37.27352858595542|3.118703571575923|11.117755224707022|8.360246283448527|4.2417508527101235|5.999518399195046|1.7769820957582605|1.0|4.0|95.58556957364911|93.93695666508452|57.4938102109297|35.88318863063039,1,200,100,0.5,2.7970831990242004
3,37.27352858595542,3.118703571575923,11.117755224707022,8.360246283448527,4.2417508527101235,5.999518399195046,1.7769820957582605,1.0,4.0,95.58556957364911,93.93695666508452,57.4938102109297,35.88318863063039,37.27352858595542|3.118703571575923|11.117755224707022|8.360246283448527|4.2417508527101235|5.999518399195046|1.7769820957582605|1.0|4.0|95.58556957364911|93.93695666508452|57.4938102109297|35.88318863063039,0,1,37.27352858595542|3.118703571575923|11.117755224707022|8.360246283448527|4.2417508527101235|5.999518399195046|1.7769820957582605|3.5601620014780373|0.6389484515148515|95.58556957364911|93.93695666508452|57.4938102109297|35.88318863063039-37.27352858595542|3.118703571575923|11.117755224707022|8.360246283448527|4.2417508527101235|5.999518399195046|1.7769820957582605|1.0|4.0|95.58556957364911|93.93695666508452|57.4938102109297|35.88318863063039,1,200,100,0.5,2.7970831990242004
4,38.87894378871176,4.983796322317,9.417488626301157,9.274998332295496,2.2688944111829494,4.728402790320414,0.2238322050160424,3.8383630973744767,0.9118912997197657,93.92086060760337,94.0459405187913,56.146730086225645,35.94815629829443,38.87894378871176|4.983796322317|9.417488626301157|9.274998332295496|2.2688944111829494|4.728402790320414|0.2238322050160424|3.8383630973744767|0.9118912997197657|93.92086060760337|94.0459405187913|56.146730086225645|35.94815629829443,0,2,38.87894378871176|4.983796322317|9.417488626301157|9.274998332295496|2.2688944111829494|4.728402790320414|0.2238322050160424|3.8383630973744767|0.9118912997197657|93.92086060760337|94.0459405187913|56.146730086225645|35.94815629829443-38.87894378871176|4.983796322317|9.417488626301157|9.274998332295496|2.2688944111829494|4.728402790320414|0.2238322050160424|1.0|4.0|93.92086060760337|94.0459405187913|56.146730086225645|35.94815629829443,1,200,100,0.5,2.7970831990242004

### Example metrics:

```json
{'DSN': 100, 'DSS': 2.7970831990242004, 'SUR': 0.5, 'TSN': 200, 'dsn_by_attr_value': {'Attr8_T=0': {'DSN': 0, 'DSS': 2.7970831990242004, 'SUR': 0, 'TSN': 0}, 'Attr8_T=1': {'DSN': 0, 'DSS': 2.7970831990242004, 'SUR': 0, 'TSN': 0}, 'Attr8_T=2': {'DSN': 0, 'DSS': 2.7970831990242004, 'SUR': 0, 'TSN': 0}, 'Attr8_T=3': {'DSN': 0, 'DSS': 2.7970831990242004, 'SUR': 0, 'TSN': 0}, 'Attr8_T=4': {'DSN': 0, 'DSS': 2.7970831990242004, 'SUR': 0, 'TSN': 0}, 'Attr9_T=0': {'DSN': 0, 'DSS': 2.7970831990242004, 'SUR': 0, 'TSN': 0}, 'Attr9_T=1': {'DSN': 0, 'DSS': 2.7970831990242004, 'SUR': 0, 'TSN': 0}, 'total': {'DSN': 200, 'TSN': 400}}, 'total_time': 279.70831990242004}
```

## Key Insights from Example

- Each discriminatory instance is represented by two rows in the DataFrame (original and modified)
- The rows are paired by the `couple_key` and `case_id`
- The `outcome` column shows that changing from Male to Female changed the prediction from 1 to 0
- The metrics show 47 discriminatory pairs found out of 1024 tested samples (4.6% success rate)
- The `dsn_by_attr_value` provides a breakdown of discrimination by protected attribute values

## Summary

The LIMI algorithm is particularly effective at finding individual fairness violations where changing protected attributes causes different model predictions despite other features remaining the same.

### **Title**
Black Box Fairness Testing of Machine Learning Models

### **Metric**
The method relies on a simplified, non-probabilistic form of **counterfactual fairness** to find discrimination.

A bias is detected if two individuals, `x` and `x'`, who differ *only* in their protected attributes (e.g., race, gender) but are identical in all other non-protected attributes, receive a different classification outcome from the machine learning model.

Formally, an individual bias instance is a pair of inputs `(x, x')` such that `M(x) ≠ M(x')`, where all non-protected attributes of `x` and `x'` are identical.

### **Individual discrimination, group, subgroup discrimination (granularity and intersectionality)**
*   **Granularity:** The method focuses exclusively on **individual discrimination**. It is designed to find specific pairs of individuals who are treated unfairly, rather than measuring statistical disparities between large demographic groups (group discrimination).
*   **Intersectionality:** The experiments in the paper were conducted by testing for one protected attribute at a time. The authors note that the approach could be extended to handle multiple protected attributes simultaneously (e.g., checking for bias against "black females"), but this would increase the computational cost as it would need to test more combinations.

### **Location**
The method tries to find discrimination in the **behavior of a trained ML model**. It operates in a **black-box** setting, meaning it does not require access to the model's internal structure, code, or original training data. It only needs API access to provide an input and receive an output. While it can leverage seed data (like training or test data) to guide its search more effectively, its goal is to audit the model, not the data itself.

### **What they find**
The method's goal is to find and report **concrete examples of individual discrimination**. It systematically generates test inputs to discover pairs of individuals (`x`, `x'`) that are identical except for a protected attribute but are assigned different labels by the model. The primary objective is to maximize the number of these discriminatory pairs found within a given testing budget (time or number of generated tests).

### **What does the method return in terms of data structure?**
The method returns a **test suite**, which is a collection of generated test inputs. This collection is implicitly divided into two parts:
1.  **Successful Test Cases (`Succ`):** A set of input pairs `(x, x')` that were found to be discriminatory. These are the concrete evidence of bias.
2.  **Generated Test Cases (`Gen`):** The total set of all unique test cases (defined by their non-protected attributes) that the algorithm generated and tested.

### **Performance**
The method's performance was evaluated by comparing it against two other fairness testing tools, **THEMIS** and **AEQUITAS**, on several benchmark datasets.

*   **Primary Evaluation Metric:** The main metric was the **success score (`#Succ / #Gen`)**, which measures the percentage of generated test cases that successfully uncovered discrimination.
*   **Results vs. THEMIS:** The proposed method (referred to as `SG`) demonstrated significantly higher performance. Across 12 benchmarks, SG achieved an average success score of **34.8%**, while THEMIS achieved **6.4%**. SG generated approximately 6 times more successful discriminatory test cases.
*   **Results vs. AEQUITAS:** SG's global and local search strategies were shown to be more effective at finding discriminatory inputs than the random sampling and perturbation methods used by AEQUITAS.
*   **Path Coverage:** The method was also evaluated on its ability to explore different decision paths within the model. It achieved **2.66 times more path coverage** than a random data-based search, indicating it more thoroughly audits the model's logic.
*   **Conclusion:** The experiments empirically demonstrate that the proposed systematic approach, combining local explainability (LIME) with symbolic execution, is significantly more effective and efficient at discovering individual discrimination in black-box models than existing random or perturbation-based methods.

# SG Algorithm Implementation

## Overview

The SG (Symbolic Generation) algorithm is a bias detection technique that uses symbolic execution and local interpretability to find discriminatory instances in machine learning models. Let me detail its input parameters and output.

## Input Parameters

The main function `run_sg()` takes the following parameters:

### 1. **data** (`DiscriminationData`)
- Required parameter
- A data object containing the dataset and metadata about protected attributes
- Contains training data, feature information, and sensitive attribute definitions

### 2. **model_type** (`str`, default='lr')
- The type of model to train
- Default is 'lr' (logistic regression)
- Other options could include 'rf' (random forest), 'dt' (decision tree), etc.

### 3. **cluster_num** (`int`, default=None)
- Number of clusters to use for K-means clustering of seed inputs
- If None, defaults to the number of unique classes in the dataset

### 4. **max_tsn** (`int`, default=100)
- Maximum number of test inputs to generate
- Algorithm terminates when this threshold is reached
- TSN stands for "Total Sample Number"

### 5. **random_seed** (`int`, default=42)
- Seed for random number generation to ensure reproducibility
- Controls randomness in clustering, model training, and LIME explanations

### 6. **max_runtime_seconds** (`int`, default=3900)
- Maximum runtime in seconds (65 minutes by default)
- Algorithm terminates when this time limit is reached

### 7. **one_attr_at_a_time** (`bool`, default=True)
- If True, checks for discrimination one protected attribute at a time
- If False, checks all protected attributes simultaneously

### 8. **db_path** (`str`, default=None)
- Optional path to a database for storing results
- If None, results are not stored in a database

### 9. **analysis_id** (`str`, default=None)
- Optional identifier for the analysis in the database
- Used when storing results in a database

### 10. **use_cache** (`bool`, default=True)
- Whether to use cached models if available
- Speeds up execution by reusing previously trained models

## Algorithm Process

The SG algorithm works through these key steps:

1. Initializes data structures to track discriminatory inputs
2. Seeds test inputs using K-means clustering on the dataset
3. Trains a machine learning model on the data
4. For each seed input:
   - Checks if it leads to discrimination
   - Uses LIME to extract decision rules from the model
   - Performs local search by flipping decision rules
   - Performs global search by systematically exploring the decision space
5. Continues until termination criteria are met (max inputs or runtime)

## Output Structure

The SG algorithm returns two main outputs:

### 1. **res_df** (`pandas.DataFrame`)

A dataframe containing pairs of discriminatory instances with the following structure:

- **Feature columns**: All columns from the original dataset (`discrimination_data.attr_columns`)
- **indv_key**: A string representation of the individual instance (concatenated feature values)
- **outcome**: The model's prediction for this instance
- **couple_key**: A key linking two instances that form a discriminatory pair
- **diff_outcome**: The absolute difference in outcomes between the pair (typically 1 for binary classification)
- **case_id**: A unique identifier for each discriminatory pair
- **Metrics columns**:
  - **TSN**: Total Sample Number (total instances tested)
  - **DSN**: Discriminatory Sample Number (total discriminatory pairs found)
  - **SUR**: Success Rate (DSN/TSN) - the proportion of tested instances that led to discrimination
  - **DSS**: Discriminatory Sample Search time (average time to find each discriminatory instance)

The dataframe contains pairs of rows with the same `case_id` and `couple_key`. Each pair represents two instances that differ only in protected attributes but receive different predictions from the model.

### 2. **metrics** (`dict`)

A dictionary containing summary statistics:

- `TSN`: Total Sample Number
- `DSN`: Discriminatory Sample Number
- `SUR`: Success Rate (DSN/TSN)
- `DSS`: Discriminatory Sample Search time
- `total_time`: Total runtime in seconds
- `dsn_by_attr_value`: Breakdown of discrimination statistics by protected attribute values

## Example Output Dataframe

Here's a more accurate example of what the output dataframe (`res_df`) would look like:

```
   sex  race  age  income  education  ... indv_key  outcome  couple_key  diff_outcome  case_id  TSN  DSN   SUR    DSS
0    1     0   45   50000          3  ... 1|0|45..      1    1|0|45..-    1            0      1000  245  0.245  12.5
1    0     0   45   50000          3  ... 0|0|45..      0    1|0|45..-    1            0      1000  245  0.245  12.5
2    1     1   32   35000          2  ... 1|1|32..      1    1|1|32..-    1            1      1000  245  0.245  12.5
3    1     0   32   35000          2  ... 1|0|32..      0    1|1|32..-    1            1      1000  245  0.245  12.5
...
```

In this example:

- Rows 0 and 1 form a discriminatory pair (case_id=0) where changing the `sex` attribute from 1 to 0 changes the prediction
- Rows 2 and 3 form another discriminatory pair (case_id=1) where changing the `race` attribute from 1 to 0 changes the prediction
- All rows contain the same metrics (TSN, DSN, SUR, DSS) which are the overall statistics for the entire analysis

The key insight is that the output dataframe contains pairs of instances that demonstrate discrimination, where each pair differs only in protected attributes but receives different predictions from the model.

# Group or Subgroup discrimination discovery methods :

### **Title**
Identifying Significant Predictive Bias in Classifiers using BiasScan

### **Metric**
The method relies on a statistical measure of **predictive bias**, which quantifies the discrepancy between a classifier's predicted outcomes and the observed outcomes for subgroups. The implementation uses the **MDSS (Multidimensional Subset Scanning)** detector with configurable scoring functions.

The core approach compares:
*   **Observations:** Actual outcomes from the dataset (`data.outcome_column`)
*   **Expectations:** Model predictions on the same data using a trained RandomForest classifier

The method performs two complementary bias scans:
1. **Overprediction Scan:** Identifies subgroups where the model predicts higher outcomes than observed
2. **Underprediction Scan:** Identifies subgroups where the model predicts lower outcomes than observed

### **Discrimination Granularity**
*   **Type:** The method is designed to find **subgroup discrimination** through multidimensional subset scanning.
*   **Granularity & Intersectionality:** The algorithm analyzes all possible intersectional combinations of attribute values across the feature space. It can identify complex, multi-dimensional subgroups defined by specific combinations of feature values (e.g., "individuals with attribute1=1, attribute2=0, attribute3=1"). The `make_products_df` function generates Cartesian products of attribute combinations to create comprehensive subgroup representations.

### **Location of Discrimination**
The method locates discrimination within the **model's predictions** by comparing predicted outcomes against actual outcomes. It performs model checking by:
- Training a RandomForest classifier on the data
- Generating predictions for all data points
- Using MDSS to identify regions where predictions systematically deviate from observations
- Scanning both overprediction and underprediction patterns

### **What They Find**
The method identifies **subgroups where the classifier exhibits systematic prediction bias**. Specifically, it finds:
1. **Overpredicted subgroups:** Where model predictions are consistently higher than actual outcomes
2. **Underpredicted subgroups:** Where model predictions are consistently lower than actual outcomes

The algorithm generates subgroup representations and calculates outcome differences from the mean to quantify bias magnitude.

### **Output Data Structure**
The method returns a tuple containing:

#### result_df (DataFrame)
Contains identified biased subgroups with columns:
- **Attribute columns:** Values defining each subgroup (from `data.attributes`)
- **outcome:** Predicted outcome for the subgroup (from `data.outcome_column`)
- **subgroup_key:** String representation of subgroup attributes (pipe-separated values)
- **diff_outcome:** Absolute difference between subgroup outcome and dataset mean outcome

#### metrics (Dictionary)
Performance metrics including:
- **TSN (Total Sample Number):** Total number of instances in the dataset
- **DSN (Detected Subgroup Number):** Number of unique subgroups identified
- **DSR (Detection Success Rate):** Ratio of detected subgroups to total samples (DSN/TSN)
- **DSS (Detection Speed Score):** Time per detected subgroup (total_time/DSN)

### **Performance**
The algorithm's performance is measured through:

*   **Computational Efficiency:**
    - Memory-efficient chunk processing in `make_products_df` (default chunk_size=10000)
    - Configurable number of iterations (`bias_scan_num_iters`, default=50)
    - Optional runtime limits (`max_runtime_seconds`)

*   **Detection Capabilities:**
    - Identifies both overprediction and underprediction bias patterns
    - Handles various data types through configurable modes ('binary', 'continuous', 'nominal', 'ordinal')
    - Supports multiple scoring functions ('Bernoulli', 'BerkJones', 'Gaussian', 'Poisson')

*   **Model Integration:**
    - Uses RandomForest classifier with configurable parameters
    - Leverages existing model training utilities (`train_sklearn_model`)
    - Supports caching for improved performance (`use_cache=True`)

### **Input Parameters**

The `run_bias_scan` function accepts:
- **data:** DiscriminationData object containing dataset, attributes, and outcome information
- **random_state:** Seed for reproducibility (default=42)
- **bias_scan_num_iters:** Number of bias scan iterations (default=50)
- **bias_scan_scoring:** Scoring function ('Poisson', 'Bernoulli', 'BerkJones', 'Gaussian')
- **bias_scan_favorable_value:** Definition of favorable outcomes ('high', 'low', or specific value)
- **bias_scan_mode:** Data type handling ('ordinal', 'binary', 'continuous', 'nominal')
- **max_runtime_seconds:** Optional runtime limit
- **use_cache:** Enable caching for performance optimization

### **Example Output**

Here's an example of what the algorithm returns:

#### result_df Example:
```
   attr1  attr2  attr3  outcome  subgroup_key  diff_outcome
0    1.0    0.0    1.0        1      1|0|1|1           0.25
1    0.0    1.0    0.0        0      0|1|0|0           0.18
2    1.0    1.0    0.0        1      1|1|0|1           0.32
3    0.0    0.0    1.0        0      0|0|1|0           0.15
4    1.0    0.0    0.0        1      1|0|0|1           0.28
```

#### metrics Example:
```python
{
    'TSN': 1000,        # Total number of instances in dataset
    'DSN': 45,          # Number of unique subgroups detected
    'DSR': 0.045,       # Detection success rate (45/1000)
    'DSS': 0.022        # Detection speed: 0.022 seconds per subgroup
}
```

**Interpretation:**
- Each row in `result_df` represents a biased subgroup identified by the algorithm
- `subgroup_key` provides a human-readable identifier for each subgroup (pipe-separated attribute values)
- `diff_outcome` shows how much each subgroup's outcome deviates from the dataset mean (higher values indicate stronger bias)
- The metrics help assess the algorithm's performance: detecting 45 biased subgroups from 1000 total instances in approximately 1 second total runtime

### **Implementation Notes**
- The algorithm processes subgroups in memory-efficient chunks to handle large combinatorial spaces
- Handles missing values by filling with median values from training data
- Removes duplicates from generated subgroup combinations
- Calculates absolute deviations from mean outcomes to quantify bias magnitude
- Supports both overprediction and underprediction bias detection in a single run

```python
from data_generator.main import generate_data
from methods.subgroup.biasscan.algo import run_bias_scan

# Generate synthetic data with controlled bias
ge = generate_data(
    nb_attributes=6,
    min_number_of_classes=2,
    max_number_of_classes=4,
    prop_protected_attr=0.1,
    nb_groups=100,
    max_group_size=100,
    categorical_outcome=True,
    nb_categories_outcome=4,
    use_cache=True
)

# Run the BiasScan algorithm
result_df, report = run_bias_scan(ge, test_size=0.3, random_state=42, n_estimators=200, bias_scan_num_iters=100,
                                  bias_scan_scoring='Poisson', bias_scan_favorable_value='high',
                                  bias_scan_mode='ordinal')

# Print results
print(result_df)
print(f"Classification Report:\n{report}")
```

The BiasScan algorithm is particularly useful for identifying specific subgroups in the data where a model exhibits biased behavior, allowing for targeted bias mitigation strategies.

# DivExplorer Algorithm Implementation

## Input Parameters

The DivExplorer algorithm is implemented in the `run_divexplorer` function in `main.py`. Here are the key input parameters:

**data**: A `DiscriminationData` object that contains:
- `training_dataframe_with_ypred`: DataFrame containing the dataset with both true labels and predicted labels
- `outcome_column`: Name of the column containing the true class labels
- `y_pred_col`: Name of the column containing the predicted class labels
- `attributes`: List of attribute names in the dataset
- `protected_attributes`: List of protected/sensitive attribute names
- `attr_columns`: List of all attribute column names

**K**: Integer (default=5) - The number of top divergent patterns to return for each metric (FPR and FNR)

**max_runtime_seconds**: Integer (default=60) - Maximum execution time in seconds before timeout

**min_support**: Float (default=0.05) - The minimum support threshold for frequent pattern mining. Only patterns that appear in at least this fraction of the dataset will be considered.

**random_state**: Integer (default=42) - Random seed for reproducibility

**use_cache**: Boolean (default=True) - Whether to use cached model training results

### Internal Parameters (used by the underlying algorithm)

When the function calls `FP_DivergenceExplorer`, it uses these parameters:

- **th_redundancy**: Integer (default=0) - Threshold for redundancy when selecting top-K patterns. Used to filter out similar patterns.

## How the Algorithm Works

1. **Model Training**: The algorithm first trains a Random Forest model using `train_sklearn_model` on the training data to generate predictions if not already available.

2. **Binary Classification Check**: It verifies that the problem is a binary classification task. If not, it returns empty results.

3. **Frequent Pattern Mining**: It initializes the `FP_DivergenceExplorer` with the dataset containing true and predicted class labels, then uses frequent pattern mining to identify patterns (subgroups) in the data with the `getFrequentPatternDivergence` method.

4. **Divergence Calculation**: For each pattern, it calculates divergence metrics:
   - **d_fpr**: Divergence in False Positive Rate - how much the FPR in this subgroup differs from the global FPR
   - **d_fnr**: Divergence in False Negative Rate - how much the FNR in this subgroup differs from the global FNR

5. **Top-K Selection**: It selects the top-K patterns with the highest absolute divergence for each metric using `getDivergenceTopKDf`.

6. **Result Processing**: It processes the results into a standardized format, filling missing attribute values with medians and generating subgroup keys.

7. **Timeout Handling**: The algorithm includes timeout functionality to prevent excessive runtime, logging a timeout message if the time limit is exceeded.

## Output

The function returns a tuple containing:

1. **result_df**: A pandas DataFrame containing the identified discriminatory subgroups with the following columns:
   - **Attribute columns**: One column for each attribute in the dataset, with values indicating the specific attribute value for this subgroup, or filled with median values if not part of the pattern
   - **outcome_column**: Predicted outcomes for the subgroup
   - **subgroup_key**: A string identifier for each subgroup using '|' as separator and '*' for wildcard values
   - **diff_outcome**: Absolute difference between subgroup outcome and mean outcome

2. **metrics**: A dictionary containing performance metrics:
   - **TSN** (Total Sample Number): Total number of instances in the dataset
   - **DSN** (Discriminatory Subgroup Number): Number of unique discriminatory subgroups found
   - **DSR** (Discriminatory Subgroup Ratio): Ratio of DSN to TSN
   - **DSS** (Discriminatory Subgroup Speed): Average time per discriminatory subgroup found

## Example

Let's say we have a dataset about loan applications with attributes like 'age', 'income', 'education', and a binary outcome 'loan_approved':

### Input Example:

```python
from data_generator.main import get_real_data, DiscriminationData
from methods.subgroup.divexplorer.main import run_divexplorer

# Get a dataset (e.g., the adult dataset)
data_obj, schema = get_real_data('adult', use_cache=False)

# Run DivExplorer with K=5 (top-5 patterns for each metric)
results_df, metrics = run_divexplorer(data_obj, K=5, max_runtime_seconds=60)
```

### Output Example:

```python
# results_df might look like:
   age  sex  education  income  marital_status  occupation  loan_approved  subgroup_key      diff_outcome
0   45    1         12   50000              1           5              1      45|1|12|50000|1|5      0.23
1   25    0          8   25000              0           3              0      25|0|8|25000|0|3       0.18
2   35    1         16   75000              1           2              1      35|1|16|75000|1|2      0.15

# metrics might look like:
{
    'TSN': 32561,      # Total samples in dataset
    'DSN': 150,        # Number of discriminatory subgroups found
    'DSR': 0.0046,     # Ratio of subgroups to total samples
    'DSS': 0.4         # Average seconds per subgroup found
}
```

## Key Features

- **Timeout Protection**: Prevents infinite runtime with configurable timeout
- **Binary Classification Focus**: Only processes binary classification problems
- **Frequent Pattern Mining**: Uses efficient pattern mining to identify subgroups
- **Dual Metric Analysis**: Analyzes both False Positive Rate and False Negative Rate divergence
- **Standardized Output**: Provides consistent output format with performance metrics
- **Robust Error Handling**: Includes fallback mechanisms and logging for debugging

This algorithm is particularly useful for identifying specific subgroups where a machine learning model exhibits biased behavior, helping to detect algorithmic discrimination in binary classification tasks.


### Title: Learning Fair Naive Bayes Classifiers by Discovering and Eliminating Discrimination Patterns

### Metric: How discrimination is measured

The algorithm uses a unified metric to detect discrimination at various levels of granularity, implemented through probabilistic inference on a trained Naive Bayes model.

*   **Core Metric (Degree of Discrimination):** The fundamental metric is the **degree of discrimination**, defined for a specific context. It measures how the probability of a positive decision `d` changes for an individual when their sensitive attributes `x` are observed, compared to when they are not (i.e., when the individual is only identified by their non-sensitive attributes `y`).
    *   Formula: `Δ(x, y) = P(d|xy) – P(d|y)`
    *   A **discrimination pattern** exists if the absolute value of this degree, `|Δ(x, y)|`, exceeds a user-defined threshold `δ` (default: 0.01).

*   **Granularity (Individual, Group, Subgroup):** This single metric can capture different fairness notions depending on the context `y`:
    *   **Group Discrimination (Statistical Parity):** If the set of non-sensitive attributes `y` is empty, the metric approximates statistical parity (`P(d|x) ≈ P(d)`).
    *   **Individual Discrimination:** If `y` includes all non-sensitive attributes, the metric captures a form of individual fairness where individuals with identical non-sensitive features should have similar outcomes regardless of their sensitive features.
    *   **Subgroup Discrimination:** By allowing `y` to be any subset of non-sensitive attributes, the method can detect discrimination within arbitrary subgroups (e.g., for a specific combination of occupation and education level).

*   **Implementation Details:** The algorithm computes probabilities `pDXY` (probability of unfavorable outcome for sensitive group) and `pD_XY` (probability of unfavorable outcome for non-sensitive group) using maximum likelihood estimation from binarized data.

### Location: Where discrimination is found

The method finds discrimination through a two-stage process combining model analysis and prediction validation.

*   **Primary Analysis:** The approach analyzes a trained **Naive Bayes classifier** built from binarized data, where all attributes are converted to binary values using median-based thresholding.
*   **Model Integration:** Unlike pure model-centric approaches, this implementation also integrates with external classifiers (Random Forest by default) to validate and predict outcomes for discovered patterns.
*   **Data Processing:** All continuous attributes are automatically binarized using median splits, and the algorithm works with the resulting binary probability distributions.
*   **Probabilistic Inference:** Discrimination patterns are identified by performing probabilistic inference on the Naive Bayes model parameters (`root_params` and `leaf_params`).

### What they find: The output of the method

The method discovers and returns specific, interpretable discrimination patterns with enhanced contextual information.

*   **What it finds:** The algorithm identifies situations where individuals receive different classification outcomes solely because their sensitive attribute was observed, along with predictions for these subgroups.
*   **Data Structure Returned:** The output is a **DataFrame containing discrimination patterns** with the following structure:
    *   `case_id`: Unique identifier for each discrimination case
    *   `discrimination_score`: Quantified measure of discrimination (pattern.score)
    *   `p_unfavorable_sensitive`: Probability of unfavorable outcome for the sensitive group
    *   `p_unfavorable_others`: Probability of unfavorable outcome for the non-sensitive group
    *   **Feature columns**: Specific assignments to both sensitive and non-sensitive attributes (None for irrelevant attributes)
    *   `nature`: Indicates whether the row represents 'base' (non-sensitive context) or 'sensitive' (sensitive attribute context)
    *   `outcome`: Predicted outcome using the trained external model
    *   `subgroup_key`: String representation of the subgroup pattern using '|' separator and '*' for wildcards
    *   `diff_outcome`: Absolute difference from mean outcome for the subgroup

### Performance: Evaluation and Results

The implementation provides comprehensive performance metrics and efficient pattern discovery.

*   **Discovery Performance (Efficiency):**
    *   **Search Efficiency:** Uses a branch-and-bound search algorithm through `PatternFinder` that explores only a fraction of the total search space
    *   **Runtime Control:** Supports configurable maximum runtime limits (`max_runtime_seconds`) for time-bounded execution
    *   **Node Tracking:** Monitors the number of nodes visited during pattern search (`nodes_visited`)

*   **Algorithm Performance Metrics:**
    *   **TSN (Total Searched Nodes):** Proxy for total sample number, representing computational effort
    *   **DSN (Discriminatory Sample Number):** Count of discovered discriminating patterns (top k patterns, default k=5)
    *   **SUR (Success Rate):** Ratio of discriminatory patterns found to total nodes searched (DSN/TSN)
    *   **DSS (Discriminatory Sample Search time):** Average time per discriminatory pattern discovery
    *   **Total Runtime:** Complete execution time from start to finish

*   **Integration Benefits:**
    *   **Model Compatibility:** Works with external ML models (Random Forest, etc.) for outcome prediction
    *   **Caching Support:** Supports model training cache for repeated experiments
    *   **Flexible Thresholds:** Configurable discrimination threshold (delta) and pattern count (k)

## Implementation

### Inputs

The Fair Naive Bayes algorithm takes a `DiscriminationData` object as its primary input, along with configuration parameters:

1. **Primary Data (`data: DiscriminationData`)**:
   - `data.dataframe`: pandas DataFrame with features, target, and protected attributes
   - `data.attr_columns`: List of feature column names
   - `data.protected_attributes`: List of sensitive/protected attribute names
   - `data.sensitive_indices`: Indices of sensitive attributes
   - `data.training_dataframe`: Subset used for model training
   - `data.outcome_column`: Name of the target/outcome column

2. **Configuration Parameters**:
   - `delta`: Discrimination threshold (default: 0.01)
   - `k`: Number of top discriminating patterns to find (default: 5)
   - `max_runtime_seconds`: Optional time limit for execution
   - `random_state`: Seed for reproducibility (default: 42)
   - `use_cache`: Enable caching for model training (default: True)

### Processing Steps

1. **External Model Training**:
   - Trains a Random Forest classifier on the original (non-binarized) data
   - Uses cross-validation and caching for efficiency
   - Model used later for outcome prediction on discovered patterns

2. **Data Binarization**:
   - Converts all attributes and target to binary values (0 or 1)
   - Uses median-based thresholding for continuous variables
   - Preserves binary attributes as-is

3. **Naive Bayes Parameter Learning**:
   - Calculates maximum likelihood parameters from binarized data
   - Creates probability dictionaries using `get_params_dict()` and `maximum_likelihood_from_data()`
   - Converts to root and leaf parameters for pattern finding

4. **Pattern Discovery**:
   - Uses `PatternFinder` with configurable parameters
   - Implements branch-and-bound search with optional time limits
   - Tracks search efficiency metrics

5. **Result Enhancement**:
   - Predicts outcomes for discovered patterns using the external model
   - Generates subgroup keys for pattern identification
   - Calculates outcome differences from population mean

### Outputs

The algorithm returns two main outputs:

1. **Enhanced Discriminating Patterns DataFrame (`res_df`)**:
   - `case_id`: Pattern identifier (grouped patterns share same ID)
   - `discrimination_score`: Quantified discrimination measure
   - `p_unfavorable_sensitive` & `p_unfavorable_others`: Probability comparisons
   - **All feature columns**: Attribute values (None for non-relevant attributes)
   - `nature`: 'base' or 'sensitive' pattern type
   - `outcome`: Predicted outcome from external model
   - `subgroup_key`: String representation (e.g., "1|0|*|1" where * = wildcard)
   - `diff_outcome`: Absolute deviation from mean outcome
   - **Performance metrics**: TSN, DSN, SUR, DSS, total_time, nodes_visited

2. **Comprehensive Metrics Dictionary (`metrics`)**:
   - `TSN`: Total Searched Nodes (computational effort proxy)
   - `DSN`: Discriminatory Sample Number (patterns found)
   - `SUR`: Success Rate (efficiency ratio)
   - `DSS`: Average time per discriminatory pattern
   - `total_time`: Total execution time
   - `nodes_visited`: Search space exploration count

### Example

For a dataset with attributes like age, income, education, and gender (protected), the algorithm:

1. **Preprocessing**: Trains Random Forest on original data, then binarizes all attributes
2. **Parameter Learning**: Calculates Naive Bayes probabilities from binary data
3. **Pattern Search**: Discovers contexts where gender affects outcomes significantly
4. **Validation**: Predicts outcomes for patterns using the Random Forest model

#### Sample Output

```
--- Discrimination Results ---
   case_id  discrimination_score  p_unfavorable_sensitive  p_unfavorable_others  age  income  education  gender  nature  outcome  subgroup_key  diff_outcome
1        1                 0.152                    0.723                 0.571    1       0          1    None   base         1       1|0|1|*        0.23
2        1                 0.152                    0.723                 0.571  None    None       None       1   sensitive     0       *|*|*|1        0.45
3        2                 0.143                    0.698                 0.555    0       1          1    None   base         1       0|1|1|*        0.18
4        2                 0.143                    0.698                 0.555  None    None       None       1   sensitive     0       *|*|*|1        0.45

--- Summary Metrics ---
TSN: 128
DSN: 2
SUR: 0.0156
DSS: 0.8745
total_time: 1.75
nodes_visited: 128
```

**Interpretation:**
- **Case 1**: High-age, low-income, high-education individuals show 15.2% discrimination based on gender
- **Pattern Keys**: "1|0|1|*" represents age=1, income=0, education=1, gender=any
- **Outcome Validation**: External model predictions confirm differential treatment
- **Efficiency**: Found 2 discriminatory patterns by exploring only 128 of potentially millions of combinations

### Title
Automated Data Slicing for Model Validation: A Big data - Al Integration Approach

### Metric
The method identifies problematic slices by analyzing the model's performance, primarily using a **loss function** (e.g., log loss). A slice is considered problematic if it meets two criteria based on this loss:

1.  **High Effect Size (φ):** The magnitude of the difference in the average loss between a slice and its counterpart (the rest of the data) must be large. The paper uses Cohen's d to quantify this, indicating how many standard deviations separate the two loss distributions.
2.  **Statistical Significance:** The difference in loss must be statistically significant. This is verified using a hypothesis test (**Welch's t-test**) to ensure the observed poor performance is not due to random chance.

### Granularity and Intersectionality
The method focuses on **subgroup discrimination**. It is designed to find underperforming slices at various levels of granularity:

*   **Group Discrimination:** It can identify simple groups where the model underperforms, defined by a single feature value (e.g., `Sex = Male`).
*   **Intersectionality ### Title
Automated Data Slicing for Model Validation: A Big Data - AI Integration Approach

### Metric
The method identifies problematic slices by analyzing the model's performance, primarily using a **loss function** (e.g., log loss). A slice is considered problematic if it meets two criteria based on this loss:

1.  **High Effect Size (φ):** The magnitude of the difference in the average loss between a slice and its counterpart (the rest of the data) must be large. The paper uses Cohen's d to quantify this, indicating how many standard deviations separate the two loss distributions.
2.  **Statistical Significance:** The difference in loss must be statistically significant. This is verified using a hypothesis test (**Welch's t-test**) to ensure the observed poor performance is not due to random chance.

### Granularity and Intersectionality
The method focuses on **subgroup discrimination**. It is designed to find underperforming slices at various levels of granularity:

*   **Group Discrimination:** It can identify simple groups where the model underperforms, defined by a single feature value (e.g., `Sex = Male`).
*   **Intersectionality (Subgroup):** The core strength of the method is finding intersectional subgroups where performance degrades. The search algorithms (Lattice Search and Decision Tree) explicitly construct slices as conjunctions of multiple feature-value pairs (e.g., `Marital Status ≠ Married-civ-spouse` AND `Capital Gain < 7298` AND `Age < 28`).

The method does not analyze discrimination at the individual level.

### Location
The method identifies discrimination within the **model's performance on a validation dataset**. It is a model validation tool that analyzes a trained model's predictions. By "slicing data to identify subsets of the validation data where the model performs poorly," it traces poor aggregate performance metrics back to specific, interpretable cohorts in the data.

### What They Find
The goal is to automatically discover and present to the user a set of **large, interpretable, and problematic data slices**.

*   **Problematic:** Slices where the model's loss is significantly higher than on the rest of the data, as determined by effect size and statistical significance.
*   **Interpretable:** Slices are defined by a simple and understandable predicate (a conjunction of a few feature-value conditions), making it easy for a human to understand the specific demographic or data cohort that is affected. This is contrasted with non-interpretable clusters.
*   **Large:** The method prioritizes larger slices, as these have a greater impact on the overall model quality and are less likely to be statistical noise.

### Data Structure Returned
The method returns a **unified DataFrame containing the top-k problematic data slices** from both approaches. The final output structure includes:

1.  **Slice Identification:** Each slice includes `slice_index`, `slice_size`, `effect_size`, and `metric` values
2.  **Feature Conditions:** For lattice slices, feature columns show the specific conditions (e.g., `≥ 30` for age)
3.  **Decision Rules:** For decision tree slices, structured as `case`, `feature`, `threshold`, `operator`, and `order`
4.  **Subgroup Keys:** Generated `subgroup_key` using pipe-separated attribute values and `diff_outcome` showing deviation from mean
5.  **Summary Metrics:** Including Total Sample Number (TSN), Discrimination Slice Number (DSN), and Discrimination Success Score (DSS)

#### Example Output DataFrame

```python
# Example of the unified results DataFrame returned by run_slicefinder()
results_df = pd.DataFrame({
    'slice_index': [0, 1, None, None, None],
    'slice_size': [1200, 800, None, None, None],
    'effect_size': [0.42, 0.38, None, None, None],
    'metric': [0.85, 0.62, None, None, None],
    'age': ['≥ 30', None, None, None, None],
    'education': [None, '≤ Bachelor', None, None, None],
    'income': ['< 50000', '< 50000', None, None, None],
    'gender': [None, 'Female', None, None, None],
    'occupation': [None, None, None, None, None],
    'marital_status': ['Married', None, None, None, None],
    'case': [None, None, 'case 1', 'case 1', 'case 2'],
    'feature': [None, None, 'age', 'income', 'education'],
    'feature_name': [None, None, 'age', 'income', 'education'],
    'threshold': [None, None, 30, 50000, 'Bachelor'],
    'operator': [None, None, '≥', '<', '≤'],
    'val_diff_outcome': [None, None, 0.42, 0.38, 0.31],
    'order': [None, None, 1, 2, 1],
    'subgroup_key': ['1|0|1|0|2', '0|1|1|1|0', '1|0|0|0|0', '1|0|0|0|0', '0|1|0|0|0'],
    'diff_outcome': [0.15, -0.08, 0.12, 0.12, -0.05],
    'outcome': [0.85, 0.62, 0.82, 0.82, 0.65]
})
```

**Interpretation of the Example:**

- **Rows 0-1:** Lattice search results showing complete subgroup definitions
  - Row 0: Married people aged ≥30 with income <$50K (1200 samples, effect size 0.42)
  - Row 1: Female with education ≤Bachelor and income <$50K (800 samples, effect size 0.38)

- **Rows 2-4:** Decision tree results showing hierarchical decision rules
  - Case 1: Two-step rule (age ≥30, then income <$50K)
  - Case 2: Single rule (education ≤Bachelor)

- **Key Columns:**
  - `subgroup_key`: Encoded representation using pipe-separated attribute values
  - `diff_outcome`: Deviation of slice performance from overall mean
  - `outcome`: Actual performance metric for each slice

### Performance Evaluation
The implementation provides two complementary approaches for slice discovery:

*   **Lattice Search (LS) Approach:**
    *   Uses parallel processing with configurable `max_workers`
    *   Controlled complexity via `degree` parameter (maximum conditions per slice)
    *   Finds overlapping problematic slices
    *   Returns slices as filter dictionaries with attribute-condition mappings

*   **Decision Tree (DT) Approach:**
    *   Uses hierarchical tree structure with configurable `dt_max_depth`
    *   Controlled by `min_size` (minimum samples per node) and `min_effect_size` thresholds
    *   Provides interpretable decision paths from root to leaf
    *   Returns structured decision rules with operators and thresholds

*   **Implementation Features:**
    *   **Timeout Protection:** Configurable `max_runtime_seconds` to prevent excessive execution time
    *   **Model Flexibility:** Accepts pre-trained models or automatically trains RandomForestClassifier
    *   **Approach Selection:** Can run "lattice", "decision_tree", or "both" approaches
    *   **Error Handling:** Graceful handling of failures in either approach
    *   **Scalability:** Efficient processing with linear scaling and parallel execution support

### Algorithm Parameters

#### Core Parameters
- **`data_obj`**: Dataset object containing features (`xdf`) and target (`ydf`)
- **`approach`**: String selecting "lattice", "decision_tree", or "both" (default: "both")
- **`model`**: Optional pre-trained model; RandomForestClassifier used if None
- **`k`**: Number of slices to return (default: 10)
- **`epsilon`**: Minimum effect size threshold (default: 0.3)

#### Model Training Parameters
- **`max_depth`**: Maximum depth for RandomForestClassifier (default: 2)
- **`n_estimators`**: Number of estimators for RandomForestClassifier (default: 1)

#### Lattice Search Parameters
- **`degree`**: Maximum complexity of slice filters (default: 2)
- **`max_workers`**: Number of parallel workers (default: 4)

#### Decision Tree Parameters
- **`dt_max_depth`**: Maximum depth for decision tree (default: 3)
- **`min_size`**: Minimum samples required to split a node (default: 100)
- **`min_effect_size`**: Minimum effect size threshold for tree slices (default: 0.3)

#### Control Parameters
- **`max_runtime_seconds`**: Timeout limit for execution (default: 60)
- **`verbose`**: Enable detailed output during execution (default: True)
- **`drop_na`**: Whether to drop missing values (default: True)

### Output Metrics
The algorithm returns comprehensive evaluation metrics:

*   **TSN (Total Sample Number):** Total number of samples in the dataset
*   **DSN (Discrimination Slice Number):** Total number of problematic slices discovered
*   **DSS (Discrimination Success Score):** Ratio DSN/TSN indicating detection efficiency
*   **Summary Statistics:** Breakdown of slices found by each approach (lattice vs decision tree)

The implementation ensures robust slice discovery through dual algorithmic approaches, comprehensive error handling, and flexible parameterization for different use cases and dataset characteristics.(Subgroup):** The core strength of the method is finding intersectional subgroups where performance degrades. The search algorithms (Lattice Search and Decision Tree) explicitly construct slices as conjunctions of multiple feature-value pairs (e.g., `Marital Status ≠ Married-civ-spouse` AND `Capital Gain < 7298` AND `Age < 28`).

The method does not analyze discrimination at the individual level.

### Location
The method identifies discrimination within the **model's performance on a validation dataset**. It is a model validation tool that analyzes a trained model's predictions. By "slicing data to identify subsets of the validation data where the model performs poorly," it traces poor aggregate performance metrics back to specific, interpretable cohorts in the data.

### What They Find
The goal is to automatically discover and present to the user a set of **large, interpretable, and problematic data slices**.

*   **Problematic:** Slices where the model's loss is significantly higher than on the rest of the data, as determined by effect size and statistical significance.
*   **Interpretable:** Slices are defined by a simple and understandable predicate (a conjunction of a few feature-value conditions), making it easy for a human to understand the specific demographic or data cohort that is affected. This is contrasted with non-interpretable clusters.
*   **Large:** The method prioritizes larger slices, as these have a greater impact on the overall model quality and are less likely to be statistical noise.

### Data Structure Returned
The method returns a **ranked list of the top-k problematic data slices**. Each element in the list represents one slice and contains:
1.  **A Predicate:** A conjunction of feature-value conditions that defines the slice (e.g., `Marital Status = Married-civ-spouse`).
2.  **Associated Metrics:** Key statistics for the slice, including its **size** (number of data points), average **log loss**, and calculated **effect size**.

This output is presented in an interactive UI with a scatter plot and a sortable table for exploration (Figure 3).

### Performance Evaluation
The performance of the proposed methods, **Lattice Search (LS)** and **Decision Tree (DT)**, was evaluated against a **Clustering (CL)** baseline.

*   **Evaluation Method:**
    *   **Accuracy:** Since the ground truth for problematic slices is unknown in real data, the authors injected known problematic slices by randomly flipping labels for certain subgroups in synthetic and real (UCI Census) datasets. Performance was then measured using **precision, recall, and accuracy (harmonic mean)** in identifying these injected slices.
    *   **Scalability:** Runtimes were measured against increasing dataset sample sizes, number of parallel workers, and number of recommendations requested (`k`).
    *   **Slice Quality:** The average effect size and average slice size of the recommended slices were compared.
    *   **False Discovery:** The effectiveness of their `α-investing` technique was compared against standard Bonferroni and Benjamini-Hochberg procedures.

*   **Results:**
    *   **Accuracy:** LS and DT significantly **outperformed** the clustering baseline. LS was generally more accurate than DT because it can find overlapping problematic slices.
    *   **Slice Quality:** LS and DT found slices with much **higher effect sizes** compared to clustering, which tended to find large but non-problematic groups.
    *   **Scalability:** The methods scaled **linearly with data size** and were effective even on very small data samples (~1% of the data), demonstrating efficiency. LS also showed improved runtime with parallelization.
    *   **Interpretability:** The slices produced by LS and DT were shown to be easily interpretable, defined by a small number of feature conditions (Table 2).

# SliceFinder Algorithm

## Implementation

SliceFinder is a bias detection algorithm that identifies subgroups (slices) in a dataset where a model performs differently compared to its overall performance. It uses two approaches: lattice search and decision tree-based search to find these interesting slices.

## Input Parameters

The main function `run_slicefinder` accepts the following parameters:

### Core Parameters

- **`data_obj`**: Object
  - Contains the dataset information with attributes:
    - `dataframe`: The complete dataset
    - `xdf`: Features dataframe
    - `ydf`: Target variable dataframe

- **`approach`**: String, default="both"
  - Which approach to use for finding slices:
    - `"lattice"`: Only use lattice search approach
    - `"decision_tree"`: Only use decision tree approach
    - `"both"`: Use both approaches

- **`model`**: Object, optional
  - Pre-trained model to use for slice finding
  - If None, a RandomForestClassifier will be trained automatically

### Model Training Parameters

- **`max_depth`**: int, default=5
  - Maximum depth for RandomForestClassifier
- **`n_estimators`**: int, default=10
  - Number of estimators for RandomForestClassifier

### Common Slice Parameters

- **`k`**: int, default=5
  - Number of slices to return
- **`epsilon`**: float, default=0.3
  - Minimum effect size threshold for considering a slice interesting

### Lattice Search Specific Parameters

- **`degree`**: int, default=2
  - Maximum complexity of slice filters (number of conditions combined)
- **`max_workers`**: int, default=4
  - Number of parallel workers for lattice search

### Decision Tree Specific Parameters

- **`dt_max_depth`**: int, default=3
  - Maximum depth for decision tree approach
- **`min_size`**: int, default=100
  - Minimum number of samples required to split a node
- **`min_effect_size`**: float, default=0.3
  - Minimum effect size threshold for decision tree slices

### Display Options

- **`verbose`**: bool, default=True
  - Whether to print detailed results during execution
- **`drop_na`**: bool, default=True
  - Whether to drop missing values from data

## Output

### Lattice Slices in Detail

The `lattice_slices` output is generated by the lattice search approach of SliceFinder. Each lattice slice represents a subgroup of the data where the model performs differently compared to its overall performance.

#### Structure of a Lattice Slice

Each `Slice` object in the lattice approach contains:

- **`filters`**: A dictionary mapping attributes to conditions
  - Format: `{attribute_name: [[[condition_value, condition_upper_bound]]]}`
  - Example: `{'age': [[[30, 45]]]}` means "30 <= age < 45"
  - Example: `{'gender': [['Male']]}` means "gender = Male"

- **`data_idx`**: The indices of data points that belong to this slice

- **`size`**: Number of data points in the slice

- **`effect_size`**: The difference in model performance for this slice compared to overall performance
  - Higher values indicate larger performance differences
  - Calculated using statistical methods comparing slice metrics to reference metrics

- **`metric`**: The actual performance metric value for this slice
  - For classification, this could be log loss, accuracy, etc.

#### Lattice Slices DataFrame

When converted to a DataFrame using `lattice_slices_to_dataframe()`, each row represents a slice with these columns:

- **`slice_index`**: Index of the slice in the original list
- **`slice_size`**: Number of samples in the slice
- **`effect_size`**: Magnitude of performance difference (higher = more interesting)
- **`metric`**: Actual performance metric value for this slice
- **Feature columns**: For each feature in the dataset:
  - If the feature is used in the slice filter: Shows the condition (e.g., "≥ 30")
  - If not used: Shows None

#### Concrete Example of Lattice Slices DataFrame

```python
# Example lattice_slices DataFrame with real-world values
lattice_slices_df = pd.DataFrame({
    'slice_index': [0, 1, 2],
    'slice_size': [1200, 800, 500],
    'effect_size': [0.42, 0.38, 0.31],
    'metric': [0.85, 0.62, 0.71],
    'age': ['≥ 30', None, '≥ 45'],
    'education': [None, '≤ Bachelor', None],
    'income': ['< 50000', '< 50000', None],
    'gender': [None, 'Female', 'Male'],
    'occupation': [None, None, 'Professional'],
    'marital_status': ['Married', None, None]
})
```

In this example:

- **Slice 0**: People who are 30 or older, married, with income less than $50,000
- **Slice 1**: Women with education up to Bachelor's degree and income less than $50,000
- **Slice 2**: Men who are 45 or older in professional occupations

### Decision Tree Slices in Detail

The `dt_slices` output is generated by the decision tree approach of SliceFinder. This approach builds a decision tree to identify regions of the feature space where model performance differs significantly.

#### Structure of Decision Tree Slices

Each decision tree slice is represented by a Node object with:

- **`desc`**: Description of the split at this node (feature and threshold)
  - Example: `['age', 30]` means a split on the 'age' feature at value 30

- **`size`**: Number of samples in this node

- **`eff_size`**: Effect size (performance difference) at this node

- **`ancestry`**: Method that returns the path from root to this node

#### Decision Tree Slices DataFrame

When converted to a DataFrame using `create_dataframe_from_nodes_for_tree_method()`, each row represents a decision rule with these columns:

- **`case`**: Case identifier (e.g., "case 1")
- **`feature`**: Feature used for the split
- **`feature_name`**: Name of the feature (same as feature)
- **`threshold`**: Threshold value for the split
- **`operator`**: Comparison operator (e.g., "<", "≥")
- **`val_diff_outcome`**: Effect size (difference in performance)
- **`order`**: Order of the split in the path (1 for root split, 2 for next level, etc.)

#### Concrete Example of Decision Tree Slices DataFrame

```python
# Example dt_slices DataFrame with real-world values
dt_slices_df = pd.DataFrame({
    'case': ['case 1', 'case 1', 'case 2', 'case 2', 'case 3'],
    'feature': ['age', 'income', 'education', 'gender', 'occupation'],
    'feature_name': ['age', 'income', 'education', 'gender', 'occupation'],
    'threshold': [30, 50000, 'Bachelor', 'Female', 'Professional'],
    'operator': ['≥', '<', '≤', '=', '='],
    'val_diff_outcome': [0.42, 0.38, 0.31, 0.29, 0.27],
    'order': [1, 2, 1, 2, 1]
})
```

In this example:

- **Case 1**: Represents a path in the decision tree where age ≥ 30 AND income < 50000
- **Case 2**: Represents a path where education ≤ Bachelor AND gender = Female
- **Case 3**: Represents a path where occupation = Professional

## Key Differences Between Lattice and Decision Tree Slices

### Representation
- **Lattice slices** represent each subgroup as a single row with all feature conditions
- **Decision tree slices** represent each decision rule as a separate row, with multiple rows forming a path

### Structure
- **Lattice slices** can have arbitrary combinations of features
- **Decision tree slices** follow hierarchical paths in a tree structure

### Interpretation
- **Lattice slices** are more intuitive for understanding complete subgroup definitions
- **Decision tree slices** better show the hierarchical importance of features

### Complexity Control
- **Lattice search** controls complexity via the `degree` parameter (max conditions per slice)
- **Decision tree approach** controls complexity via `dt_max_depth` (max depth of tree)

Both approaches aim to identify subgroups with significant performance differences, but they use different algorithms to explore the feature space and may find different interesting slices in the data.

### **Title**
SliceLine: Fast, Linear-Algebra-based Slice Finding for ML Model Debugging

### **Metric**
The method relies on a custom, flexible **Scoring Function** to find problematic data slices. This score quantifies how much worse a model performs on a specific slice compared to its average performance, while also considering the size of the slice.

The score `sc` for a slice `S` is defined as:
`sc = α * ( (se / |S|) / ē - 1 ) - (1 - α) * ( n / |S| - 1 )`

Where:
*   `se / |S|` is the average error on the slice.
*   `ē` is the average error on the entire dataset.
*   `|S|` is the size (number of rows) of the slice.
*   `n` is the size of the entire dataset.
*   `α` is a user-defined weight parameter (between 0 and 1) that balances the importance of the slice's error versus its size. A higher `α` prioritizes slices with high error, even if they are small.

A score `sc > 0` indicates that the model performs worse on the slice than on the overall dataset. The goal is to find slices that maximize this score.

### **Individual discrimination, group, subgroup discrimination**
SliceLine is designed to find **subgroup discrimination** with a high degree of **granularity and intersectionality**.

*   It does not focus on individual discrimination (i.e., comparing one individual to another).
*   It identifies **groups** (or "slices") defined by the conjunction of multiple feature predicates. For example, it can find that a model underperforms for the subgroup where `gender = female` AND `degree = PhD`.
*   By searching through combinations of features, it inherently uncovers intersectional biases that might be missed when looking at single features in isolation.
*   **The implementation specifically targets two types of errors**: False Positive Rate (FPR) and False Negative Rate (FNR) discrimination, running separate analyses for each error type.

### **Location**
The method tries to find discrimination by analyzing a **trained model's performance on the data**. It does not modify the model or the training process itself. It operates as a post-hoc debugging tool that takes a model's predictions (and resulting errors) on a dataset and searches for problematic subsets within that data.

**Implementation Details:**
- Uses a Random Forest model (`model_type='rf'`) as the base classifier when training is needed
- Operates on the training dataframe with predicted outcomes (`training_dataframe_with_ypred`)
- Calculates error vectors for both False Positives and False Negatives separately
- Runs two independent SliceFinder instances, one for each error type

### **What they find**
The method finds the **top-K problematic data slices** for both False Positive and False Negative errors. A "slice" is a subset of the data defined by a conjunction of predicates on its features. A "problematic" slice is one where a trained ML model performs significantly worse (i.e., has a higher error rate) than its average performance across the entire dataset, according to the scoring function.

**Specific Error Types:**
- **False Positive slices**: Subgroups where the model incorrectly predicts positive outcomes (y_true=0, y_pred=1)
- **False Negative slices**: Subgroups where the model incorrectly predicts negative outcomes (y_true=1, y_pred=0)

The method aims to find discriminated groups (subgroups) relative to the average performance, not to find discriminated individuals.

### **What does the method return in terms of data structure?**
The algorithm returns a **pandas DataFrame** containing the combined top-K slices from both FPR and FNR analyses, along with a **metrics dictionary**.

**DataFrame Structure:**
1. **Feature columns**: Values indicating the conditions that define each slice
   - For categorical features: specific values (e.g., "female", "PhD")
   - For numerical features: threshold values (e.g., ">30", "<25")
   - `None` values indicate "don't care" features not part of the slice definition

2. **Statistical columns** added by the algorithm:
   - **score**: The calculated score for the slice based on the scoring function
   - **error**: Total error in the slice (sum of errors)
   - **error_rate**: Average error rate in the slice (error / slice size)
   - **size**: Number of rows in the slice
   - **metric**: Either 'fpr' (False Positive Rate) or 'fnr' (False Negative Rate)
   - **subgroup_key**: String representation of the slice conditions
   - **diff_outcome**: Difference from mean outcome
   - **slice**: Human-readable description of the slice conditions

**Metrics Dictionary:**
- **RUNTIME**: Total execution time in seconds
- **TSN**: Total Sample Number (total input samples tested)
- **DSN**: Discriminatory Sample Number (samples in found slices)
- **SUR**: Success Rate (DSN/TSN ratio)
- **DSS**: Discriminatory Sample Search time (RUNTIME/DSN ratio)

### **Performance**
The performance was evaluated on its effectiveness (pruning), efficiency (runtime), and scalability using a variety of real-world datasets (Adult, Covtype, KDD98, US Census, Criteo).

**Implementation Performance Characteristics:**
*   **Default Parameters**: K=5 (top slices), alpha=0.95 (error-focused), max_l=3 (max predicates), max_runtime_seconds=60
*   **Error Handling**: Gracefully handles cases where no significant slices are found
*   **Caching**: Supports model training cache to improve repeated runs
*   **Memory Management**: Uses pandas DataFrames for efficient data manipulation
*   **Timeout Protection**: Built-in maximum runtime limit to prevent infinite execution

*   **Evaluation Environment**: Experiments were run on a powerful scale-up server (112 virtual cores, 768 GB RAM) and a scale-out cluster. The method was implemented in Apache SystemDS, which leverages efficient sparse linear algebra.
*   **Pruning Effectiveness**: The paper shows that the combination of size pruning, score pruning, and handling of parent-child relationships in the search lattice is crucial. Without these techniques, the enumeration of slices becomes computationally infeasible even on small datasets.
*   **End-to-end Runtime & Scalability**:
    *   **Local Performance**: The method is very fast. On the Adult dataset, it completed in **5.6 seconds**, which is significantly faster than the >100s reported for the original SliceFinder work on the same dataset.
    *   **Scalability with Rows**: The method showed excellent scalability with the number of rows, demonstrating near-linear performance degradation as the USCensus dataset was replicated up to 10 times (to ~25M rows).
    *   **Scalability with Columns**: On the massive Criteo dataset (192M rows, 76M one-hot encoded columns), SliceLine was able to effectively enumerate slices and completed in under 45 minutes in a distributed environment, demonstrating its ability to handle extremely high-dimensional, sparse data.

### **Result DataFrame Example**

Here's an example of the actual output DataFrame structure returned by SliceLine:

```csv
   Attr1_X  Attr2_X  Attr3_X  Attr4_X  Attr5_X  Attr6_X  Attr7_X  Attr8_T  Attr9_T  Attr10_X  Attr11_X  Attr12_X  Attr13_X  slice_score        sum_slice_error  max_slice_error  slice_size  slice_average_error  metric  subgroup_key                      outcome  diff_outcome  slice
0     None     None     None       14     None     None        0     None     None      None      None        67      None          NaN                NaN              NaN         NaN                  NaN     fpr  *|*|*|14|*|*|0|*|*|*|*|67|*           0        -0.125  Attr4_X=14, Attr7_X=0, Attr12_X=67
1     None     None        9     None     None     None        0     None     None      None      None        67      None          NaN                NaN              NaN         NaN                  NaN     fpr  *|*|9|*|*|*|0|*|*|*|*|67|*            0        -0.125  Attr3_X=9, Attr7_X=0, Attr12_X=67
2      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN       NaN       NaN       NaN       NaN    4.748395               66.0              1.0      1470.0            0.044898     fpr  *|*|*|*|*|*|*|*|*|*|*|*|*             0        -0.125  slice_score=4.75, sum_slice_error=66.0
3      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN       NaN       NaN       NaN       NaN    4.748395               66.0              1.0      1470.0            0.044898     fpr  *|*|*|*|*|*|*|*|*|*|*|*|*             0        -0.125  slice_score=4.75, sum_slice_error=66.0
4     None     None     None        7     None     None        0     None     None      None      None        67      None          NaN                NaN              NaN         NaN                  NaN     fnr  *|*|*|7|*|*|0|*|*|*|*|67|*            1         0.875  Attr4_X=7, Attr7_X=0, Attr12_X=67
```

**Key aspects of the result DataFrame:**

- **Feature columns (Attr1_X - Attr13_X)**: Show the specific conditions defining each slice
  - `None`: Feature not part of the slice definition (don't care)
  - `NaN`: Used when no specific slice conditions are found
  - Specific values (e.g., `14`, `9`, `0`, `67`): Exact conditions for the slice

- **Statistical columns**:
  - `slice_score`: The calculated discrimination score (higher = more problematic)
  - `sum_slice_error`: Total number of errors in the slice
  - `max_slice_error`: Maximum error value in the slice
  - `slice_size`: Number of data points in the slice
  - `slice_average_error`: Average error rate within the slice

- **Metadata columns**:
  - `metric`: Either 'fpr' (False Positive Rate) or 'fnr' (False Negative Rate)
  - `subgroup_key`: Pipe-separated representation of slice conditions (`*` = don't care)
  - `outcome`: Predicted outcome for this slice (0 or 1)
  - `diff_outcome`: Difference from the overall mean outcome
  - `slice`: Human-readable description of the slice conditions

**Interpretation:**
- Rows 0, 1: Specific slices with precise feature conditions but NaN scores (may indicate insufficient data)
- Rows 2, 3: General slices with high discrimination scores (4.75) affecting large populations (1470 samples)
- Row 4: False Negative slice with different feature conditions

### **Usage Example**
```python
from data_generator.main import get_real_data

# Load data
data_obj, schema = get_real_data('adult', use_cache=False)

# Run sliceline
results_df, metrics = run_sliceline(
    data=data_obj,
    K=5,                    # Number of top slices to find
    alpha=0.95,             # Weight parameter (0-1)
    max_l=3,                # Max predicates per slice
    max_runtime_seconds=60, # Timeout limit
    random_state=42,        # Reproducibility
    use_cache=True,         # Enable model caching
    logger=logger           # Optional logging
)

if not results_df.empty:
    print("Top Slices found by Sliceline:")
    print(results_df[['slice', 'slice_score', 'slice_average_error', 'slice_size', 'metric']])
    print(f"Metrics: {metrics}")
```

### Title: Probabilistic Verification of Fairness Properties via Concentration

### **Metric: What metric does the method rely on to find discrimination?**

The method uses a **flexible specification language** that can express various fairness definitions through arithmetic and logical constraints. Based on the implementation, the core metric is:

* **Ratio-based Fairness Assessment:** The algorithm computes the ratio of positive outcomes between two demographic groups and compares it against a fairness threshold `c` (default: 0.15 or 15%).
* **Demographic Parity:** The primary implementation focuses on demographic parity, verifying that minority members receive favorable outcomes at a rate within an acceptable threshold of the majority group rate.
* **Mutual Information Scoring:** The algorithm uses mutual information scores between protected attributes and outcomes to prioritize which attributes to analyze first, focusing on those most likely to exhibit discrimination.

The method supports additional fairness metrics through its specification language framework, including Equal Opportunity, Path-Specific Causal Fairness, and Individual Fairness.

### **Location: Where does this method try to find discrimination?**

The method finds discrimination in **trained machine learning models** when applied to test datasets. The algorithm operates on:

1. **Trained Scikit-learn Models**: The implementation uses a `SklearnModelSampler` wrapper that makes any trained scikit-learn classifier compatible with VeriFair's verification process.
2. **Test Dataset Subgroups**: The algorithm partitions the test data based on protected attribute values (e.g., Male vs Female for gender) and analyzes the model's behavior on each subgroup.
3. **All Protected Attribute Combinations**: The method systematically examines all possible pairwise combinations of values within each protected attribute, ordered by their mutual information scores with the outcome variable.

The verification process treats the ML model as a black box and focuses on its decision-making behavior across different demographic groups.

### **What they find: What exactly does this method try to find?**

The method performs **systematic fairness verification** across all combinations of protected attributes:

1. **Prioritized Analysis**: Uses mutual information scores to rank protected attributes by their likelihood of exhibiting discrimination, analyzing the most suspicious attributes first.
2. **Pairwise Group Comparisons**: For each protected attribute, compares all possible pairs of attribute values (e.g., Male vs Female, White vs Black vs Asian for all combinations).
3. **Binary Fairness Determination**: For each comparison, determines whether the model violates the specified fairness threshold between the two groups.
4. **Comprehensive Coverage**: Analyzes all protected attributes and their value combinations rather than requiring pre-specified groups of interest.

### **What does the method return in terms of data structure?**

The method returns a **pandas DataFrame** where each row represents a demographic subgroup from the pairwise comparisons, containing:

**Attribute Columns:**
* Multiple protected attribute columns (e.g., `Attr1_X`, `Attr2_X`, `Attr1_T`, `Attr2_T`, etc.): Each column corresponds to a protected attribute, with specific values for the subgroup being analyzed or `<NA>` for non-relevant attributes
* The naming convention appears to distinguish between different types of attributes (with `_X` and `_T` suffixes)

**Subgroup Identification:**
* `subgroup_key`: Pipe-separated string identifier showing the specific values for each attribute position (e.g., `*|*|*|*|*|*|*|*|*|*|2|*|*|*|*|*|*|*|*|*` where `2` indicates the value for that attribute and `*` indicates non-relevant attributes)
* `group_key`: Combined identifier for the pairwise comparison, showing both subgroups being compared (e.g., `*|*|...|2|*|...-*|*|...|0|*|...`)

**Discrimination Measures:**
* `diff_outcome`: Numerical difference in outcome rates between the compared groups (can be positive or negative, e.g., `0.22830024346212374`, `-0.10470630905662404`)

**Performance Metrics (repeated for each row):**
* `TSN`: Total Sample Number across all analyses (e.g., `7378`)
* `DSN`: Discriminatory Sample Number - count of unfair results found (e.g., `2`)
* `SUR`: Success/Unfairness Rate calculated as DSN/TSN (e.g., `0.0002710761724044456`)
* `DSS`: Discriminatory Sample Search time - time per discrimination found in seconds (e.g., `154.72836637496948`)
* `total_time`: Total execution time in seconds (e.g., `309.45673274993896`)
* `nodes_visited`: Total samples processed, matching TSN (e.g., `7378`)

**Data Structure Characteristics:**
* Each pairwise comparison generates **two rows** - one for each subgroup in the comparison
* The same performance metrics are replicated across all rows from a single analysis run
* Most attribute columns contain `<NA>` values, with only the relevant attribute for each comparison containing actual values
* The `subgroup_key` uses a positional encoding where each position corresponds to a specific attribute

### **Performance: How has this method's performance been evaluated and what was the result?**

The implementation includes several performance optimizations and evaluation mechanisms:

1. **Scalability Features:**
   * **Timeout Management**: Global `max_runtime_seconds` parameter prevents excessive runtime
   * **Iterative Sampling**: Configurable `n_samples` and `n_max` parameters control memory usage and convergence
   * **Progress Logging**: Regular progress updates every `log_iters` iterations

2. **Efficiency Optimizations:**
   * **Mutual Information Prioritization**: Analyzes attributes most likely to show discrimination first
   * **Early Stopping**: Can halt analysis when time limits are reached
   * **Sampling Strategy**: Uses sampling with replacement to handle varying group sizes efficiently

3. **Robustness Measures:**
   * **Statistical Confidence**: Extremely low default error probability (δ = 0.5 × 10⁻¹⁰)
   * **Empty Group Handling**: Returns neutral outcomes for empty demographic groups
   * **Convergence Detection**: Identifies when analysis fails to converge

4. **Comprehensive Metrics:**
   * **Coverage Metrics**: TSN and DSN provide quantitative measures of analysis breadth
   * **Efficiency Metrics**: SUR and DSS measure discrimination detection efficiency
   * **Time Tracking**: Detailed timing information for performance assessment

The method demonstrates strong performance characteristics with configurable trade-offs between accuracy, runtime, and statistical confidence, making it suitable for both quick assessments and thorough fairness audits.


I have built an algorithm that generates synthetic data for testing discrimination discovery algorithms :

\section{Proposed Method: A Phased Approach to Synthetic Data Generation}

We propose a novel algorithm for generating synthetic datasets specifically designed to benchmark discrimination discovery methods. The core of our approach is a four-phase process that uses advanced statistical modeling (Gaussian Copula) to generate realistic data while providing precise control over the properties of injected bias. The framework defines demographic groups as pairs of subgroups that differ in controlled ways, allowing for the systematic evaluation of a method's ability to detect discrimination across varying levels of intersectionality, group similarity, and data uncertainty. This enables rigorous testing under conditions that are difficult or impossible to achieve with real-world data alone.

\subsection{Core Concepts and Data Structure}

To understand our generation process, we first define its key components. The final dataset is organized hierarchically, as illustrated in Figure~\ref{fig:group_subgroup_overview}.

\textbf{Schema ($\mathcal{S}$):} The foundational blueprint of the dataset. It defines the set of protected attributes $\mathcal{T} = \{T_1, \ldots, T_p\}$ (e.g., race, gender) and unprotected attributes $\mathcal{X} = \{X_1, \ldots, X_q\}$ (e.g., education, occupation). Each attribute has a defined set of possible values.

\textbf{Subgroup ($S$):} A collection of individuals who share a specific combination of values across a defined set of protected and non-protected attributes. A subgroup is defined by a set of constraints, for example, $\{\text{Race}=\text{Black}, \text{Gender}=\text{Female}, \text{Education}=\text{Bachelors}\}$. Subgroups are the fundamental unit of generation in our framework.

\textbf{Group ($G$):} The primary unit for analysis. In our framework, a group is conceptually a pair of subgroups, $G = \{S_1, S_2\}$, that are generated to be comparable. For instance, they might share the same non-protected attributes but differ in a protected attribute, allowing for a direct test of fairness.

\textbf{Individual ($I$):} A single data point (or row) in the dataset, characterized by a full set of attribute values and an assigned outcome label, $y_I$.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{imgs/Overview of groups and subgroups.png}
\caption{Hierarchical structure of the generated synthetic dataset. The dataset consists of multiple groups, each containing a pair of comparable subgroups composed of individuals.}
\label{fig:group_subgroup_overview}
\end{figure}

\subsection{Framework Architecture and Algorithm}

Our synthetic data generation framework operates in four distinct phases, as detailed in Algorithm~\ref{alg:synthetic_data_generation} and illustrated in Figure~\ref{fig:synthetic_data_generation_process}.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{imgs/synthetic_data_generation_process.png}
\caption{The four-phase synthetic data generation framework. The process begins with synthesizer training, proceeds through the definition and population of subgroup pairs, and concludes with outcome generation and dataset assembly.}
\label{fig:synthetic_data_generation_process}
\end{figure}

\subsubsection{Phase 1: Synthesizer Training}

The process begins by training a probabilistic generative model to learn the underlying correlations and distributions of the data. We use a Gaussian Copula model for its ability to capture complex, non-linear dependencies. The framework supports two modes:

\textbf{Descriptive Mode:} When a real-world reference dataset (e.g., Adult, COMPAS) is available, the synthesizer is trained directly on this empirical data to learn its statistical properties (Algorithm~\ref{alg:synthetic_data_generation}, line 3).

\textbf{Prescriptive Mode:} If no real data is used, the framework can generate a large seed dataset from an abstract schema defining desired correlations and marginal distributions, and then train the synthesizer on this seed data (lines 5--6).

\subsubsection{Phase 2: Subgroup Pair Definition}

Instead of defining groups directly, we define pairs of comparable subgroups. For each pair, we randomly sample a set of protected and non-protected attributes to define its intersectionality and granularity (lines 10--14). The first subgroup, $SG_A$, is assigned specific values for these attributes. The second subgroup, $SG_B$, is then created by modifying the values of $SG_A$ based on a similarity parameter $s$, which controls the fraction of attribute values that remain the same (lines 16--19). This allows us to create pairs of subgroups that are arbitrarily similar or different, providing fine-grained control for testing.

\subsubsection{Phase 3: Independent Population Generation}

Each defined subgroup is populated independently by drawing samples from the trained synthesizer using conditional sampling (line 24). This crucial step ensures that all individuals generated for a subgroup adhere to its defining constraints (e.g., all have Race=White) while their other attributes are populated in a way that remains statistically consistent with the joint distribution learned in Phase 1. This preserves realistic correlations within the data.

\subsubsection{Phase 4: Outcome Generation and Dataset Assembly}

In the final phase, we generate an outcome variable for each individual and assemble the final dataset. This is where bias and uncertainty are precisely injected:

\textbf{Baseline Prediction:} A baseline outcome is calculated using a linear model (line 32).

\textbf{Bias Injection:} A specific, predetermined bias term, $\text{bias}_{SG}$, is added to the outcome for all individuals within a given subgroup (line 33). This creates the ground-truth discrimination that we later task discovery methods with finding.

\textbf{Uncertainty Modeling:} We inject two forms of uncertainty to mimic real-world complexity. Epistemic uncertainty (model uncertainty) is simulated by creating an ensemble of models around the base model and measuring the variance in their predictions (lines 36--38). Aleatoric uncertainty (inherent data noise) is added as random Gaussian noise (line 41).

\textbf{Assembly:} Each individual is stored as a record containing its attributes, final outcome, subgroup identifier, and ground-truth annotations for the injected bias and uncertainty levels (line 44).

While the algorithm generates distinct subgroup pairs, the final assembled dataset allows for flexible analysis. Any subgroup can be compared against any other, allowing the concept of a ``group'' to emerge dynamically from the data, which is especially useful for complex intersectional studies.

\begin{algorithm}[H]
\caption{Synthetic Data Generation with Controlled Discriminatory Groups}
\label{alg:synthetic_data_generation}
\begin{algorithmic}[1]
\Require
    $n_{pairs}$: Number of subgroup pairs to generate
    \Statex \hspace{\algorithmicindent} $n_{samples}$: Samples per subgroup
    \Statex \hspace{\algorithmicindent} $D_{real}$ or $\mathcal{S}$: Optional real data or abstract schema
    \Statex \hspace{\algorithmicindent} $\{\text{bias}_S\}$: Pre-defined bias magnitudes for subgroups
\Ensure
    $\mathcal{D}_{synth}$: Synthetic dataset with ground truth annotations

\Statex \textbf{Phase 1: Synthesizer Training}
\If{$D_{real}$ is provided}
    \State $\theta \leftarrow \text{FitGenerativeModel}(D_{real})$ \Comment{Learn distribution from real data}
\Else
    \State $D_{seed} \leftarrow \text{GenerateFromSchema}(\mathcal{S})$ \Comment{Create data from abstract rules}
    \State $\theta \leftarrow \text{FitGenerativeModel}(D_{seed})$
\EndIf

\Statex
\Statex \textbf{Phase 2: Subgroup Pair Definition}
\State $\mathcal{G}_{pairs} \leftarrow \emptyset$ \Comment{Initialize collection of subgroup pairs}
\For{$i = 1$ to $n_{pairs}$}
    \State $s \sim \text{Uniform}(0, 1)$ \Comment{Sample a similarity parameter for the pair}
    \State $\text{attrs}_{def} \leftarrow \text{SampleDefiningAttributes}(\mathcal{T}, \mathcal{X})$ \Comment{Select attrs for intersectionality}
    \State $S_A \leftarrow \text{DefineSubgroup}(\text{attrs}_{def})$ \Comment{Define first subgroup's constraints}
    \State $S_B \leftarrow \text{CreateSimilarSubgroup}(S_A, s)$ \Comment{Define second, based on similarity $s$}
    \State $\mathcal{G}_{pairs} \leftarrow \mathcal{G}_{pairs} \cup \{(S_A, S_B)\}$
\EndFor

\Statex
\Statex \textbf{Phase 3: Independent Population Generation}
\State $\mathcal{P} \leftarrow \emptyset$ \Comment{Initialize collection of populated subgroups}
\For{each $(S_A, S_B) \in \mathcal{G}_{pairs}$}
    \State $P_A \leftarrow \text{ConditionalSample}(\theta, S_A, n_{samples})$ \Comment{Generate individuals for $S_A$}
    \State $P_B \leftarrow \text{ConditionalSample}(\theta, S_B, n_{samples})$ \Comment{Generate individuals for $S_B$}
    \State $\mathcal{P} \leftarrow \mathcal{P} \cup \{(P_A, S_A), (P_B, S_B)\}$
\EndFor

\Statex
\Statex \textbf{Phase 4: Outcome Generation \& Dataset Assembly}
\State $\mathcal{D}_{synth} \leftarrow \emptyset$
\State $W, b \leftarrow \text{InitializeLinearModel}()$ \Comment{Base model for outcome generation}
\For{each $(P_S, S) \in \mathcal{P}$}
    \For{each individual $\mathbf{x} \in P_S$}
        \State $y_{base} \leftarrow W\mathbf{x} + b + \text{bias}_{S}$ \Comment{Inject subgroup-specific ground truth bias}

        \State $\sigma_{epistemic} \leftarrow \text{EstimateModelUncertainty}(\mathbf{x}, W)$ \Comment{e.g., via MC-Dropout}
        \State $\epsilon_{aleatoric} \sim \mathcal{N}(0, \sigma_{alea}^2)$ \Comment{Inherent data noise}
        \State $y_{final} \leftarrow \sigma(y_{base} + \epsilon_{aleatoric})$ \Comment{Apply sigmoid to get final probability}

        \State \text{record} \leftarrow (\mathbf{x}, y_{final}, S, \text{bias}_{S}, \sigma_{epistemic}, \sigma_{aleatoric})$
        \State $\mathcal{D}_{synth} \leftarrow \mathcal{D}_{synth} \cup \{\text{record}\}$
    \EndFor
\EndFor

\State \Return $\mathcal{D}_{synth}$
\end{algorithmic}
\end{algorithm}


Help me write the outline for an arxiv article that would test these methods for discrimination discovery using my synthetic data generation algorithm