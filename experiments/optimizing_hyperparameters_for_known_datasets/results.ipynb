{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-17T19:55:16.221837Z",
     "start_time": "2025-05-17T19:55:16.177563Z"
    }
   },
   "source": [
    "from path import HERE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from IPython.display import display\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "DB_PATH = HERE.joinpath(\"methods/optimization/optimizations.db\")\n",
    "\n",
    "\n",
    "def load_optimization_results(db_path):\n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"Database file not found at {db_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Query to get all optimization trials\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        id, timestamp, study_name, trial_number, method, \n",
    "        parameters, metrics, runtime, extra_data, generation_arguments\n",
    "    FROM \n",
    "        optimization_trials\n",
    "    ORDER BY \n",
    "        timestamp DESC, study_name, trial_number\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Parse JSON columns\n",
    "    for col in ['parameters', 'metrics', 'extra_data', 'generation_arguments']:\n",
    "        df[col] = df[col].apply(lambda x: json.loads(x) if x else {})\n",
    "\n",
    "    # Extract key information from nested JSON\n",
    "    df['SUR'] = df['metrics'].apply(lambda x: x.get('SUR', 0))\n",
    "    df['DSN'] = df['metrics'].apply(lambda x: x.get('DSN', 0))\n",
    "    df['TSN'] = df['metrics'].apply(lambda x: x.get('TSN', 0))\n",
    "\n",
    "    # Extract dataset and data source from extra_data\n",
    "    df['dataset'] = df['extra_data'].apply(lambda x: x.get('dataset', 'unknown'))\n",
    "    df['data_source'] = df['extra_data'].apply(lambda x: x.get('data_source', 'unknown'))\n",
    "\n",
    "    # Parse timestamps\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    df['study_parts'] = df['study_name'].str.split('_')\n",
    "    df['algorithm'] = df['method']\n",
    "\n",
    "    df['experiment_id'] = df['study_name'].apply(\n",
    "        lambda x: '_'.join(x.split('_')[:-1]) if x and '_' in x else x\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading optimization results...\")\n",
    "results_df = load_optimization_results(DB_PATH)\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"No results found in the database. Please check the database path.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(results_df)} optimization trials\")\n",
    "\n",
    "    # Show basic statistics\n",
    "    print(\"\\nData sources:\", results_df['data_source'].unique())\n",
    "    print(\"Datasets:\", results_df['dataset'].unique())\n",
    "    print(\"Algorithms:\", results_df['algorithm'].unique())\n",
    "\n",
    "    # Show the first few rows\n",
    "    print(\"\\nSample of the loaded data:\")\n",
    "    display(\n",
    "        results_df[['study_name', 'trial_number', 'algorithm', 'dataset', 'data_source', 'SUR', 'DSN', 'TSN']].head(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading optimization results...\n",
      "Loaded 545 optimization trials\n",
      "\n",
      "Data sources: ['real' 'pure-synthetic' 'unknown']\n",
      "Datasets: ['adult' 'synthetic_30_20' 'synthetic_2_2' 'synthetic_15_15'\n",
      " 'synthetic_3_10' 'synthetic_20_3' 'synthetic_10_5' 'synthetic_10_12'\n",
      " 'synthetic_10_8' 'synthetic_10_3' 'synthetic_20_5' 'synthetic_5_5'\n",
      " 'synthetic_2_5' 'synthetic_50_5' 'bank' 'credit']\n",
      "Algorithms: ['sg' 'adf' 'aequitas' 'expga']\n",
      "\n",
      "Sample of the loaded data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                     study_name  trial_number algorithm  \\\n",
       "0                      real_adult_sg_1747508093             6        sg   \n",
       "1                      real_adult_sg_1747508093             3        sg   \n",
       "2                      real_adult_sg_1747508093             2        sg   \n",
       "3                      real_adult_sg_1747508093             1        sg   \n",
       "4                      real_adult_sg_1747508093             0        sg   \n",
       "5       experiment_1747006358_config_23_adf_opt             2       adf   \n",
       "6       experiment_1747006358_config_23_adf_opt             1       adf   \n",
       "7       experiment_1747006358_config_23_adf_opt             0       adf   \n",
       "8  experiment_1747006358_config_23_aequitas_opt             2  aequitas   \n",
       "9  experiment_1747006358_config_23_aequitas_opt             1  aequitas   \n",
       "\n",
       "           dataset     data_source       SUR   DSN    TSN  \n",
       "0            adult            real  0.156979  1372   8740  \n",
       "1            adult            real  0.023901   318  13305  \n",
       "2            adult            real  0.236758  2512  10610  \n",
       "3            adult            real  0.091997  1161  12620  \n",
       "4            adult            real  0.056680  1134  20007  \n",
       "5  synthetic_30_20  pure-synthetic  0.318039  6363  20007  \n",
       "6  synthetic_30_20  pure-synthetic  0.335183  6706  20007  \n",
       "7  synthetic_30_20  pure-synthetic  0.339131  6785  20007  \n",
       "8  synthetic_30_20  pure-synthetic  0.182786  3657  20007  \n",
       "9  synthetic_30_20  pure-synthetic  0.208853  3232  15475  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_name</th>\n",
       "      <th>trial_number</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>dataset</th>\n",
       "      <th>data_source</th>\n",
       "      <th>SUR</th>\n",
       "      <th>DSN</th>\n",
       "      <th>TSN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>real_adult_sg_1747508093</td>\n",
       "      <td>6</td>\n",
       "      <td>sg</td>\n",
       "      <td>adult</td>\n",
       "      <td>real</td>\n",
       "      <td>0.156979</td>\n",
       "      <td>1372</td>\n",
       "      <td>8740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>real_adult_sg_1747508093</td>\n",
       "      <td>3</td>\n",
       "      <td>sg</td>\n",
       "      <td>adult</td>\n",
       "      <td>real</td>\n",
       "      <td>0.023901</td>\n",
       "      <td>318</td>\n",
       "      <td>13305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>real_adult_sg_1747508093</td>\n",
       "      <td>2</td>\n",
       "      <td>sg</td>\n",
       "      <td>adult</td>\n",
       "      <td>real</td>\n",
       "      <td>0.236758</td>\n",
       "      <td>2512</td>\n",
       "      <td>10610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>real_adult_sg_1747508093</td>\n",
       "      <td>1</td>\n",
       "      <td>sg</td>\n",
       "      <td>adult</td>\n",
       "      <td>real</td>\n",
       "      <td>0.091997</td>\n",
       "      <td>1161</td>\n",
       "      <td>12620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>real_adult_sg_1747508093</td>\n",
       "      <td>0</td>\n",
       "      <td>sg</td>\n",
       "      <td>adult</td>\n",
       "      <td>real</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>1134</td>\n",
       "      <td>20007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>experiment_1747006358_config_23_adf_opt</td>\n",
       "      <td>2</td>\n",
       "      <td>adf</td>\n",
       "      <td>synthetic_30_20</td>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>0.318039</td>\n",
       "      <td>6363</td>\n",
       "      <td>20007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>experiment_1747006358_config_23_adf_opt</td>\n",
       "      <td>1</td>\n",
       "      <td>adf</td>\n",
       "      <td>synthetic_30_20</td>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>0.335183</td>\n",
       "      <td>6706</td>\n",
       "      <td>20007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>experiment_1747006358_config_23_adf_opt</td>\n",
       "      <td>0</td>\n",
       "      <td>adf</td>\n",
       "      <td>synthetic_30_20</td>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>0.339131</td>\n",
       "      <td>6785</td>\n",
       "      <td>20007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>experiment_1747006358_config_23_aequitas_opt</td>\n",
       "      <td>2</td>\n",
       "      <td>aequitas</td>\n",
       "      <td>synthetic_30_20</td>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>0.182786</td>\n",
       "      <td>3657</td>\n",
       "      <td>20007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>experiment_1747006358_config_23_aequitas_opt</td>\n",
       "      <td>1</td>\n",
       "      <td>aequitas</td>\n",
       "      <td>synthetic_30_20</td>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>0.208853</td>\n",
       "      <td>3232</td>\n",
       "      <td>15475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T19:55:21.365883Z",
     "start_time": "2025-05-17T19:55:21.334447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to analyze best results per experiment\n",
    "def analyze_best_results():\n",
    "    \"\"\"Extract and display the best results for each experiment\"\"\"\n",
    "    if results_df.empty:\n",
    "        print(\"No results available\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Group by experiment and find the best SUR value\n",
    "    # Use (data_source, dataset, algorithm) as the experiment identifier\n",
    "    best_results = []\n",
    "\n",
    "    # Get unique experiments\n",
    "    experiments = results_df.groupby(['data_source', 'dataset', 'algorithm'])\n",
    "\n",
    "    for name, group in experiments:\n",
    "        data_source, dataset, algorithm = name\n",
    "\n",
    "        # Find the row with the highest SUR\n",
    "        best_idx = group['SUR'].idxmax()\n",
    "        best_row = group.loc[best_idx]\n",
    "\n",
    "        # Extract relevant parameters\n",
    "        best_params = best_row['parameters']\n",
    "\n",
    "        # Keep only the most relevant parameters\n",
    "        relevant_params = {}\n",
    "        for param in ['threshold_rank', 'max_global', 'max_local', 'model_type',\n",
    "                      'cross_rate', 'mutation', 'cluster_num', 'step_size']:\n",
    "            if param in best_params:\n",
    "                relevant_params[param] = best_params[param]\n",
    "\n",
    "        best_results.append({\n",
    "            'data_source': data_source,\n",
    "            'dataset': dataset,\n",
    "            'algorithm': algorithm,\n",
    "            'best_SUR': best_row['SUR'],\n",
    "            'DSN': best_row['DSN'],\n",
    "            'TSN': best_row['TSN'],\n",
    "            'runtime': best_row['runtime'],\n",
    "            'trial_number': best_row['trial_number'],\n",
    "            'relevant_params': relevant_params,\n",
    "            'study_name': best_row['study_name']\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    best_df = pd.DataFrame(best_results)\n",
    "\n",
    "    # Sort by best SUR descending\n",
    "    best_df = best_df.sort_values(by='best_SUR', ascending=False)\n",
    "\n",
    "    return best_df\n",
    "\n",
    "\n",
    "# Analyze and display best results\n",
    "best_results_df = analyze_best_results()\n",
    "\n",
    "if not best_results_df.empty:\n",
    "    print(f\"Best results across {len(best_results_df)} experiments:\\n\")\n",
    "\n",
    "    # Display the results without the parameters column to keep it readable\n",
    "    display_cols = ['data_source', 'dataset', 'algorithm', 'best_SUR', 'DSN', 'TSN', 'runtime']\n",
    "    display(best_results_df[display_cols])\n",
    "\n",
    "    # Show details of the top 3 performing configurations\n",
    "    print(\"\\nTop 3 performing configurations:\")\n",
    "    for i, row in best_results_df.head(3).iterrows():\n",
    "        print(f\"\\n{i + 1}. {row['data_source']} - {row['dataset']} - {row['algorithm']}\")\n",
    "        print(f\"   SUR: {row['best_SUR']:.4f}, DSN: {row['DSN']}, TSN: {row['TSN']}\")\n",
    "        print(\"   Parameters:\")\n",
    "        for param, value in row['relevant_params'].items():\n",
    "            print(f\"     {param}: {value}\")\n",
    "else:\n",
    "    print(\"No results to analyze\")"
   ],
   "id": "e75080199c880471",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results across 62 experiments:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       data_source          dataset algorithm  best_SUR   DSN    TSN  \\\n",
       "35  pure-synthetic    synthetic_2_5        sg  0.765957   108    141   \n",
       "34  pure-synthetic    synthetic_2_5     expga  0.763889   110    144   \n",
       "7   pure-synthetic   synthetic_10_3        sg  0.750000     6      8   \n",
       "23  pure-synthetic   synthetic_20_3        sg  0.750000     6      8   \n",
       "4   pure-synthetic   synthetic_10_3       adf  0.750000     6      8   \n",
       "..             ...              ...       ...       ...   ...    ...   \n",
       "49            real            adult        sg  0.236758  2512  10610   \n",
       "38  pure-synthetic  synthetic_30_20     expga  0.216877  2529  11661   \n",
       "37  pure-synthetic  synthetic_30_20  aequitas  0.208853  3232  15475   \n",
       "50         unknown            adult       adf  0.186718  1746   9351   \n",
       "44  pure-synthetic   synthetic_50_5     expga  0.000000     0      0   \n",
       "\n",
       "       runtime  \n",
       "35   13.844282  \n",
       "34  100.257659  \n",
       "7     3.184629  \n",
       "23    3.491005  \n",
       "4   100.017163  \n",
       "..         ...  \n",
       "49  734.088364  \n",
       "38  106.782654  \n",
       "37  108.599256  \n",
       "50  605.093658  \n",
       "44  300.406900  \n",
       "\n",
       "[62 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_source</th>\n",
       "      <th>dataset</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>best_SUR</th>\n",
       "      <th>DSN</th>\n",
       "      <th>TSN</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>synthetic_2_5</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>108</td>\n",
       "      <td>141</td>\n",
       "      <td>13.844282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>synthetic_2_5</td>\n",
       "      <td>expga</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>110</td>\n",
       "      <td>144</td>\n",
       "      <td>100.257659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>synthetic_10_3</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3.184629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>synthetic_20_3</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3.491005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>synthetic_10_3</td>\n",
       "      <td>adf</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>100.017163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>real</td>\n",
       "      <td>adult</td>\n",
       "      <td>sg</td>\n",
       "      <td>0.236758</td>\n",
       "      <td>2512</td>\n",
       "      <td>10610</td>\n",
       "      <td>734.088364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>synthetic_30_20</td>\n",
       "      <td>expga</td>\n",
       "      <td>0.216877</td>\n",
       "      <td>2529</td>\n",
       "      <td>11661</td>\n",
       "      <td>106.782654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>synthetic_30_20</td>\n",
       "      <td>aequitas</td>\n",
       "      <td>0.208853</td>\n",
       "      <td>3232</td>\n",
       "      <td>15475</td>\n",
       "      <td>108.599256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>unknown</td>\n",
       "      <td>adult</td>\n",
       "      <td>adf</td>\n",
       "      <td>0.186718</td>\n",
       "      <td>1746</td>\n",
       "      <td>9351</td>\n",
       "      <td>605.093658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>pure-synthetic</td>\n",
       "      <td>synthetic_50_5</td>\n",
       "      <td>expga</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>300.406900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 7 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 performing configurations:\n",
      "\n",
      "36. pure-synthetic - synthetic_2_5 - sg\n",
      "   SUR: 0.7660, DSN: 108, TSN: 141\n",
      "   Parameters:\n",
      "     model_type: rf\n",
      "     cluster_num: 149\n",
      "\n",
      "35. pure-synthetic - synthetic_2_5 - expga\n",
      "   SUR: 0.7639, DSN: 110, TSN: 144\n",
      "   Parameters:\n",
      "     threshold_rank: 0.11646759543664197\n",
      "     max_global: 19429\n",
      "     max_local: 425\n",
      "     model_type: rf\n",
      "     cross_rate: 0.5955525998052242\n",
      "     mutation: 0.09545624180177516\n",
      "\n",
      "8. pure-synthetic - synthetic_10_3 - sg\n",
      "   SUR: 0.7500, DSN: 6, TSN: 8\n",
      "   Parameters:\n",
      "     model_type: rf\n",
      "     cluster_num: 149\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T19:58:21.202986Z",
     "start_time": "2025-05-17T19:58:21.040461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy import stats\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "query = \"SELECT method, parameters, metrics FROM optimization_trials\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Parse JSON columns\n",
    "df['parameters'] = df['parameters'].apply(json.loads)\n",
    "df['metrics'] = df['metrics'].apply(json.loads)\n",
    "df['SUR'] = df['metrics'].apply(lambda x: x.get('SUR', 0))\n",
    "\n",
    "# Analyze each method\n",
    "methods = ['expga', 'sg', 'aequitas', 'adf']\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\n===== {method.upper()} HYPERPARAMETER CORRELATION WITH SUR =====\")\n",
    "    \n",
    "    # Filter for this method\n",
    "    method_df = df[df['method'] == method].copy()\n",
    "    \n",
    "    if len(method_df) < 5:\n",
    "        print(f\"Not enough data for {method}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Number of trials: {len(method_df)}\")\n",
    "    \n",
    "    # Get all parameters for this method\n",
    "    all_params = set()\n",
    "    for params in method_df['parameters']:\n",
    "        all_params.update(params.keys())\n",
    "    \n",
    "    # Exclude non-hyperparameters\n",
    "    exclude_params = ['max_runtime_seconds', 'max_tsn', 'use_cache', 'use_gpu', 'random_seed']\n",
    "    params = [p for p in all_params if p not in exclude_params]\n",
    "    \n",
    "    # Create dataframe with parameters and SUR\n",
    "    param_df = pd.DataFrame({'SUR': method_df['SUR'].values})\n",
    "    \n",
    "    # Extract parameters to columns\n",
    "    for param in params:\n",
    "        param_df[param] = method_df['parameters'].apply(lambda x: x.get(param, None))\n",
    "    \n",
    "    # Convert to numeric where possible\n",
    "    for col in param_df.columns:\n",
    "        if col != 'SUR':\n",
    "            try:\n",
    "                param_df[col] = pd.to_numeric(param_df[col])\n",
    "            except:\n",
    "                # Keep as is if not numeric\n",
    "                pass\n",
    "    \n",
    "    # Calculate correlation and p-value for each parameter\n",
    "    results = []\n",
    "    \n",
    "    for param in params:\n",
    "        if param in param_df.columns and pd.api.types.is_numeric_dtype(param_df[param]):\n",
    "            # Clean data by removing NaN values\n",
    "            clean_data = param_df[['SUR', param]].dropna()\n",
    "            \n",
    "            if len(clean_data) > 5:  # Need enough data points\n",
    "                # Calculate Pearson correlation and p-value\n",
    "                corr, p_value = stats.pearsonr(clean_data[param], clean_data['SUR'])\n",
    "                \n",
    "                # Add to results\n",
    "                results.append({\n",
    "                    'parameter': param,\n",
    "                    'correlation': corr,\n",
    "                    'p_value': p_value,\n",
    "                    'n': len(clean_data)\n",
    "                })\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    results.sort(key=lambda x: abs(x['correlation']), reverse=True)\n",
    "    \n",
    "    # Print results with significance markers\n",
    "    print(f\"{'Parameter':<20} {'Correlation':<12} {'P-Value':<12} {'N':<6} {'Sig'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for result in results:\n",
    "        # Add significance markers\n",
    "        sig = ''\n",
    "        if result['p_value'] < 0.001:\n",
    "            sig = '***'\n",
    "        elif result['p_value'] < 0.01:\n",
    "            sig = '**'\n",
    "        elif result['p_value'] < 0.05:\n",
    "            sig = '*'\n",
    "        \n",
    "        print(f\"{result['parameter']:<20} {result['correlation']:>10.4f}  {result['p_value']:>10.4f}  {result['n']:>4}   {sig}\")\n",
    "    \n",
    "    print(\"\\nSignificance levels: * p<0.05, ** p<0.01, *** p<0.001\")"
   ],
   "id": "6b43ca0c1836f53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EXPGA HYPERPARAMETER CORRELATION WITH SUR =====\n",
      "Number of trials: 149\n",
      "Parameter            Correlation  P-Value      N      Sig\n",
      "------------------------------------------------------------\n",
      "mutation                 0.2405      0.1299    41   \n",
      "max_local               -0.2371      0.1355    41   \n",
      "one_attr_at_a_time      -0.1402      0.3819    41   \n",
      "max_global               0.1299      0.4182    41   \n",
      "cross_rate               0.1246      0.4377    41   \n",
      "threshold_rank           0.0880      0.5842    41   \n",
      "\n",
      "Significance levels: * p<0.05, ** p<0.01, *** p<0.001\n",
      "\n",
      "===== SG HYPERPARAMETER CORRELATION WITH SUR =====\n",
      "Number of trials: 140\n",
      "Parameter            Correlation  P-Value      N      Sig\n",
      "------------------------------------------------------------\n",
      "one_attr_at_a_time      -0.2523      0.1376    36   \n",
      "cluster_num              0.2478      0.1451    36   \n",
      "\n",
      "Significance levels: * p<0.05, ** p<0.01, *** p<0.001\n",
      "\n",
      "===== AEQUITAS HYPERPARAMETER CORRELATION WITH SUR =====\n",
      "Number of trials: 131\n",
      "Parameter            Correlation  P-Value      N      Sig\n",
      "------------------------------------------------------------\n",
      "init_prob                0.4102      0.0465    24   *\n",
      "one_attr_at_a_time       0.3495      0.0941    24   \n",
      "max_global              -0.1754      0.4123    24   \n",
      "param_probability_change_size    -0.0590      0.7840    24   \n",
      "max_local                0.0419      0.8460    24   \n",
      "direction_probability_change_size    -0.0252      0.9070    24   \n",
      "step_size               -0.0114      0.9579    24   \n",
      "\n",
      "Significance levels: * p<0.05, ** p<0.01, *** p<0.001\n",
      "\n",
      "===== ADF HYPERPARAMETER CORRELATION WITH SUR =====\n",
      "Number of trials: 125\n",
      "Parameter            Correlation  P-Value      N      Sig\n",
      "------------------------------------------------------------\n",
      "max_global               0.3830      0.0403    29   *\n",
      "max_local                0.1387      0.4730    29   \n",
      "step_size                0.1272      0.5109    29   \n",
      "cluster_num             -0.1248      0.5188    29   \n",
      "one_attr_at_a_time       0.0687      0.7234    29   \n",
      "\n",
      "Significance levels: * p<0.05, ** p<0.01, *** p<0.001\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T20:07:15.707092Z",
     "start_time": "2025-05-17T20:07:14.239864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy import stats\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "# Get trials with dataset information\n",
    "query = \"SELECT method, parameters, metrics, extra_data FROM optimization_trials\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Parse JSON columns\n",
    "df['parameters'] = df['parameters'].apply(json.loads)\n",
    "df['metrics'] = df['metrics'].apply(json.loads)\n",
    "df['extra_data'] = df['extra_data'].apply(json.loads)\n",
    "df['SUR'] = df['metrics'].apply(lambda x: x.get('SUR', 0))\n",
    "\n",
    "# Extract dataset information\n",
    "df['dataset'] = df['extra_data'].apply(lambda x: x.get('dataset', 'unknown'))\n",
    "df['data_source'] = df['extra_data'].apply(lambda x: x.get('data_source', 'unknown'))\n",
    "\n",
    "# Print basic statistics\n",
    "print(f\"Total trials: {len(df)}\")\n",
    "print(f\"Methods: {', '.join(df['method'].unique())}\")\n",
    "print(f\"Datasets: {', '.join(df['dataset'].unique())}\")\n",
    "print(f\"Data sources: {', '.join(df['data_source'].unique())}\\n\")\n",
    "\n",
    "# Function to analyze parameter consistency across datasets\n",
    "def analyze_consistency(method_name):\n",
    "    \"\"\"Analyze how parameters correlate with SUR across different datasets\"\"\"\n",
    "    # Filter for the specific method\n",
    "    method_df = df[df['method'] == method_name].copy()\n",
    "    \n",
    "    # Get all unique parameters for this method\n",
    "    all_params = set()\n",
    "    for params in method_df['parameters']:\n",
    "        all_params.update(params.keys())\n",
    "    \n",
    "    # Exclude non-hyperparameters\n",
    "    exclude_params = ['max_runtime_seconds', 'max_tsn', 'use_cache', 'use_gpu', 'random_seed']\n",
    "    params = [p for p in all_params if p not in exclude_params]\n",
    "    \n",
    "    # Get unique datasets with enough trials\n",
    "    dataset_counts = method_df['dataset'].value_counts()\n",
    "    valid_datasets = dataset_counts[dataset_counts >= 5].index.tolist()\n",
    "    \n",
    "    if len(valid_datasets) < 2:\n",
    "        print(f\"Not enough datasets with sufficient trials for {method_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Store correlations for each dataset\n",
    "    dataset_correlations = {}\n",
    "    \n",
    "    # Process each dataset\n",
    "    for dataset in valid_datasets:\n",
    "        dataset_df = method_df[method_df['dataset'] == dataset]\n",
    "        \n",
    "        # Store correlations for this dataset\n",
    "        correlations = {}\n",
    "        p_values = {}\n",
    "        sample_sizes = {}\n",
    "        \n",
    "        # Process each parameter\n",
    "        for param in params:\n",
    "            # Create temporary dataframe with SUR and parameter\n",
    "            param_values = []\n",
    "            for _, row in dataset_df.iterrows():\n",
    "                param_values.append(row['parameters'].get(param, None))\n",
    "            \n",
    "            temp_df = pd.DataFrame({\n",
    "                'SUR': dataset_df['SUR'].values,\n",
    "                'param': param_values\n",
    "            })\n",
    "            \n",
    "            # Convert parameter to numeric if possible\n",
    "            try:\n",
    "                temp_df['param'] = pd.to_numeric(temp_df['param'])\n",
    "                is_numeric = True\n",
    "            except:\n",
    "                is_numeric = False\n",
    "            \n",
    "            # Calculate correlation if parameter is numeric\n",
    "            if is_numeric:\n",
    "                # Remove NaN values\n",
    "                clean_df = temp_df.dropna()\n",
    "                \n",
    "                if len(clean_df) >= 5:  # Need at least 5 data points\n",
    "                    corr, p_val = stats.pearsonr(clean_df['param'], clean_df['SUR'])\n",
    "                    \n",
    "                    correlations[param] = corr\n",
    "                    p_values[param] = p_val\n",
    "                    sample_sizes[param] = len(clean_df)\n",
    "        \n",
    "        # Store results for this dataset if we have correlations\n",
    "        if correlations:\n",
    "            dataset_correlations[dataset] = {\n",
    "                'correlations': correlations,\n",
    "                'p_values': p_values,\n",
    "                'sample_sizes': sample_sizes\n",
    "            }\n",
    "    \n",
    "    if len(dataset_correlations) < 2:\n",
    "        print(f\"Not enough datasets with valid correlations for {method_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Analyze consistency across datasets\n",
    "    print(f\"\\n{method_name.upper()} - Analysis across {len(dataset_correlations)} datasets\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get all parameters that have correlations in at least 2 datasets\n",
    "    shared_params = set()\n",
    "    for dataset, data in dataset_correlations.items():\n",
    "        for param in data['correlations'].keys():\n",
    "            if sum(1 for d in dataset_correlations.values() if param in d['correlations']) >= 2:\n",
    "                shared_params.add(param)\n",
    "    \n",
    "    if not shared_params:\n",
    "        print(\"No parameters shared across multiple datasets.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a table of correlations across datasets\n",
    "    consistency_data = []\n",
    "    \n",
    "    for param in shared_params:\n",
    "        # Collect data for this parameter\n",
    "        param_data = {\n",
    "            'parameter': param,\n",
    "            'datasets': 0,\n",
    "            'significant_datasets': 0,\n",
    "            'consistent_sign': True,\n",
    "            'correlations': [],\n",
    "            'p_values': [],\n",
    "            'sign': None\n",
    "        }\n",
    "        \n",
    "        # Process each dataset\n",
    "        for dataset, data in dataset_correlations.items():\n",
    "            if param in data['correlations']:\n",
    "                corr = data['correlations'][param]\n",
    "                p_val = data['p_values'][param]\n",
    "                \n",
    "                param_data['datasets'] += 1\n",
    "                param_data['correlations'].append(corr)\n",
    "                param_data['p_values'].append(p_val)\n",
    "                \n",
    "                # Check if significant\n",
    "                if p_val < 0.05:\n",
    "                    param_data['significant_datasets'] += 1\n",
    "                \n",
    "                # Check sign consistency\n",
    "                current_sign = np.sign(corr)\n",
    "                if param_data['sign'] is None:\n",
    "                    param_data['sign'] = current_sign\n",
    "                elif param_data['sign'] != current_sign and current_sign != 0:\n",
    "                    param_data['consistent_sign'] = False\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        param_data['mean_corr'] = np.mean(param_data['correlations'])\n",
    "        param_data['std_corr'] = np.std(param_data['correlations'])\n",
    "        param_data['min_corr'] = min(param_data['correlations'])\n",
    "        param_data['max_corr'] = max(param_data['correlations'])\n",
    "        param_data['corr_range'] = param_data['max_corr'] - param_data['min_corr']\n",
    "        \n",
    "        # Add to results\n",
    "        consistency_data.append(param_data)\n",
    "    \n",
    "    # Sort by consistency and strength\n",
    "    consistency_data.sort(key=lambda x: (\n",
    "        x['consistent_sign'], \n",
    "        x['significant_datasets'], \n",
    "        abs(x['mean_corr'])\n",
    "    ), reverse=True)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{'Parameter':<20} {'Mean Corr':<10} {'Range':<15} {'Sign':<15} {'Significant'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for data in consistency_data:\n",
    "        sign_text = \"Consistent\" if data['consistent_sign'] else \"Inconsistent\"\n",
    "        range_text = f\"[{data['min_corr']:.3f}, {data['max_corr']:.3f}]\"\n",
    "        sig_text = f\"{data['significant_datasets']}/{data['datasets']}\"\n",
    "        \n",
    "        print(f\"{data['parameter']:<20} {data['mean_corr']:>9.3f}  {range_text:<15} {sign_text:<15} {sig_text}\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    consistent_params = [d for d in consistency_data if d['consistent_sign'] and d['significant_datasets'] > 0]\n",
    "    sig_consistent_params = [d for d in consistent_params if d['significant_datasets'] >= 2]\n",
    "    \n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"- {len(consistent_params)}/{len(consistency_data)} parameters have consistent direction across datasets\")\n",
    "    print(f\"- {len(sig_consistent_params)}/{len(consistency_data)} parameters are significant and consistent across multiple datasets\")\n",
    "    \n",
    "    if sig_consistent_params:\n",
    "        print(\"\\nMost consistently effective parameters:\")\n",
    "        for i, data in enumerate(sig_consistent_params[:3]):\n",
    "            direction = \"positively\" if data['mean_corr'] > 0 else \"negatively\"\n",
    "            print(f\"{i+1}. {data['parameter']} ({direction} correlated, significant in {data['significant_datasets']}/{data['datasets']} datasets)\")\n",
    "    \n",
    "    return consistency_data\n",
    "\n",
    "# Analyze each method\n",
    "methods = ['expga', 'sg', 'aequitas', 'adf']\n",
    "\n",
    "for method in methods:\n",
    "    analyze_consistency(method)"
   ],
   "id": "ea27dea69f80b738",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trials: 546\n",
      "Methods: sg, expga, adf, aequitas\n",
      "Datasets: adult, credit, bank, synthetic_50_5, synthetic_2_5, synthetic_10_5, synthetic_5_5, synthetic_20_5, synthetic_10_3, synthetic_10_8, synthetic_10_12, synthetic_20_3, synthetic_3_10, synthetic_15_15, synthetic_2_2, synthetic_30_20\n",
      "Data sources: unknown, pure-synthetic, real\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gen06917\\AppData\\Local\\Temp\\ipykernel_7240\\3329541512.py:89: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, p_val = stats.pearsonr(clean_df['param'], clean_df['SUR'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPGA - Analysis across 8 datasets\n",
      "--------------------------------------------------------------------------------\n",
      "Parameter            Mean Corr  Range           Sign            Significant\n",
      "--------------------------------------------------------------------------------\n",
      "cross_rate                 nan  [-0.821, 0.865] Inconsistent    4/8\n",
      "threshold_rank             nan  [-0.521, 0.884] Inconsistent    2/8\n",
      "mutation                   nan  [-0.400, 0.474] Inconsistent    0/8\n",
      "max_global                 nan  [-0.517, 0.436] Inconsistent    0/8\n",
      "one_attr_at_a_time         nan  [nan, nan]      Inconsistent    0/8\n",
      "max_local                  nan  [-0.592, 0.550] Inconsistent    0/8\n",
      "\n",
      "Summary:\n",
      "- 0/6 parameters have consistent direction across datasets\n",
      "- 0/6 parameters are significant and consistent across multiple datasets\n",
      "\n",
      "SG - Analysis across 7 datasets\n",
      "--------------------------------------------------------------------------------\n",
      "Parameter            Mean Corr  Range           Sign            Significant\n",
      "--------------------------------------------------------------------------------\n",
      "one_attr_at_a_time         nan  [nan, nan]      Inconsistent    1/7\n",
      "cluster_num                nan  [-0.308, 0.801] Inconsistent    0/7\n",
      "\n",
      "Summary:\n",
      "- 0/2 parameters have consistent direction across datasets\n",
      "- 0/2 parameters are significant and consistent across multiple datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gen06917\\AppData\\Local\\Temp\\ipykernel_7240\\3329541512.py:89: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, p_val = stats.pearsonr(clean_df['param'], clean_df['SUR'])\n",
      "C:\\Users\\gen06917\\AppData\\Local\\Temp\\ipykernel_7240\\3329541512.py:89: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, p_val = stats.pearsonr(clean_df['param'], clean_df['SUR'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AEQUITAS - Analysis across 7 datasets\n",
      "--------------------------------------------------------------------------------\n",
      "Parameter            Mean Corr  Range           Sign            Significant\n",
      "--------------------------------------------------------------------------------\n",
      "step_size               -0.297  [-0.658, 0.320] Inconsistent    1/7\n",
      "max_global               0.153  [-0.465, 0.739] Inconsistent    1/7\n",
      "one_attr_at_a_time         nan  [nan, nan]      Inconsistent    0/7\n",
      "max_local               -0.219  [-0.600, 0.032] Inconsistent    0/7\n",
      "param_probability_change_size     0.179  [-0.080, 0.698] Inconsistent    0/7\n",
      "init_prob               -0.171  [-0.680, 0.226] Inconsistent    0/7\n",
      "direction_probability_change_size     0.035  [-0.389, 0.527] Inconsistent    0/7\n",
      "\n",
      "Summary:\n",
      "- 0/7 parameters have consistent direction across datasets\n",
      "- 0/7 parameters are significant and consistent across multiple datasets\n",
      "\n",
      "ADF - Analysis across 7 datasets\n",
      "--------------------------------------------------------------------------------\n",
      "Parameter            Mean Corr  Range           Sign            Significant\n",
      "--------------------------------------------------------------------------------\n",
      "max_local                0.307  [-0.784, 0.969] Inconsistent    3/7\n",
      "max_global              -0.041  [-0.587, 0.833] Inconsistent    2/7\n",
      "step_size               -0.296  [-0.984, 0.387] Inconsistent    1/7\n",
      "one_attr_at_a_time         nan  [nan, nan]      Inconsistent    1/7\n",
      "cluster_num              0.299  [-0.038, 0.999] Inconsistent    1/7\n",
      "\n",
      "Summary:\n",
      "- 0/5 parameters have consistent direction across datasets\n",
      "- 0/5 parameters are significant and consistent across multiple datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gen06917\\AppData\\Local\\Temp\\ipykernel_7240\\3329541512.py:89: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, p_val = stats.pearsonr(clean_df['param'], clean_df['SUR'])\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
