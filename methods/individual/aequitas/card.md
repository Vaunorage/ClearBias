### Title: AEQUITAS : Automated Directed Fairness Testing

### Metric: What metric does the method rely on to find discrimination?
The method, AEQUITAS, relies on **individual fairness**. The paper states, "we focus towards individual fairness, as it is critical for eliminating societal bias and aim to check for discrimination that might violate individual fairness [2]."

This is formally defined in **Definition 4.1 (Discriminatory Input and fairness)**, where an input `I` is considered discriminatory if another input `I'`, which is identical to `I` except for its values on protected attributes (e.g., gender), receives a significantly different output from the model (`|f(I) − f(I')| > γ`).

### Location: Where does this method try to find discrimination models or data?
The method finds discrimination in the **machine learning model** itself. It operates as a black-box testing approach. The abstract clearly states, "For a given machine-learning model and a set of sensitive input parameters, our AEQUITAS approach automatically discovers discriminatory inputs that highlight fairness violation." The methodology involves systematically generating inputs and observing the model's outputs to uncover biased behavior, rather than analyzing the training data directly.

### What they find: What exactly does this method try to find and what does the method return?
- **What it finds**: The method aims to find **individual discriminatory inputs**. These are specific input examples that, when compared to a minimally different counterpart (differing only on a sensitive attribute), receive a different classification from the model, thus exposing a fairness violation.

- **What it returns**: The method returns a **set of discriminatory inputs**. The algorithms described in the paper (e.g., `GLOBAL_EXP` and `LOCAL_EXP`) build and return a set (`disc_inps` or `Test`) containing the inputs that were found to cause discriminatory behavior in the model.

### Performance: How did this method's performance been evaluated and what was the result?
The performance of AEQUITAS was evaluated against six classifiers (including SVM, Random Forest, and a "Fair SVM") on the US census income dataset, with "gender" as the sensitive attribute.

The evaluation and results were as follows:

1.  **Effectiveness (Finding Discrimination)**: AEQUITAS significantly outperformed purely random testing. It was more effective by "a factor of 9.6 on average and up to a factor of 20.4". In its most advanced configuration (`fully-directed`), it generated test suites where up to **70% of the inputs were discriminatory**.

2.  **Efficiency (Speed)**: The method was much faster than random testing at finding discriminatory inputs. The `fully-directed` version was **83.27% faster on average** and up to **96.62% faster** in the best case.

3.  **Utility (Improving Fairness)**: The discriminatory inputs generated by AEQUITAS were successfully used to retrain and improve the models. This automated retraining process reduced the percentage of discriminatory inputs by an **average of 43.2%** across the tested models, with a maximum reduction of **94.36%** for the Decision Tree classifier.

Implementation
Aequitas Algorithm: Input and Output Analysis
Input
The Aequitas algorithm is a bias detection method that works by searching for individual discrimination in machine learning models. It takes the following inputs:

DiscriminationData object (
data
): Contains:
training_dataframe: The dataset used to train the model
protected_attributes: List of sensitive attributes (e.g., race, gender, age)
outcome_column: Target variable name
attr_columns: All feature column names
sensitive_indices: Indices of protected attributes in the feature list
input_bounds
: Min and max values for each feature
Algorithm parameters:
model_type: The type of model to train (default: 'rf' for Random Forest)
max_global: Maximum number of global samples to test (default: 500)
max_local: Maximum number of local search iterations (default: 2000)
step_size: Step size for local search (default: 1.0)
max_runtime_seconds: Maximum runtime in seconds (default: 3600)
max_tsn: Maximum number of samples to test (default: 10000)
one_attr_at_a_time: Whether to vary one attribute at a time (default: False)
Algorithm Process
Training: The algorithm first trains a model using the provided training data.
Global Discrimination Discovery: It randomly samples instances from the dataset and tests them for discrimination.
Local Discrimination Discovery: For each discriminatory instance found in the global phase, it performs a local search to find more discriminatory instances in the neighborhood.
Discrimination Testing: For each instance, it creates variants by changing the protected attributes and checks if the model's prediction changes.
Output
The algorithm returns two main outputs:

Results DataFrame (res_df): A pandas DataFrame containing all discovered discriminatory pairs with the following columns:
All feature columns from the original dataset
indv_key: A unique identifier for each individual instance
outcome: The model's prediction for this instance
couple_key: A unique identifier for each pair of instances
diff_outcome: The absolute difference in outcomes between the pair
case_id: A unique identifier for each discrimination case
TSN: Total Sample Number - total number of instances tested
DSN: Discriminatory Sample Number - number of discriminatory instances found
SUR: Success Rate - ratio of DSN to TSN
DSS: Discriminatory Sample Search time - average time to find each discriminatory instance
Metrics Dictionary (
metrics
): Contains:
TSN: Total Sample Number
DSN: Discriminatory Sample Number
SUR: Success Rate (DSN/TSN)
DSS: Discriminatory Sample Search time
total_time: Total execution time
dsn_by_attr_value: Discrimination statistics broken down by protected attribute
Example Output DataFrame
Here's an example of what the output DataFrame might look like for a credit dataset with protected attributes like age and gender:

   age  gender  income  credit_score  loan_amount  outcome  indv_key    couple_key  diff_outcome  case_id   TSN   DSN    SUR     DSS
0   35       0   65000          720        50000        1  35|0|65000|720|50000  35|0|65000|720|50000-35|1|65000|720|50000       1        0  3500   127  0.036  28.346
1   35       1   65000          720        50000        0  35|1|65000|720|50000  35|0|65000|720|50000-35|1|65000|720|50000       1        0  3500   127  0.036  28.346
2   42       0   72000          680        75000        1  42|0|72000|680|75000  42|0|72000|680|75000-42|1|72000|680|75000       1        1  3500   127  0.036  28.346
3   42       1   72000          680        75000        0  42|1|72000|680|75000  42|0|72000|680|75000-42|1|72000|680|75000       1        1  3500   127  0.036  28.346
4   28       0   45000          650        30000        0  28|0|45000|650|30000  28|0|45000|650|30000-28|1|45000|650|30000       1        2  3500   127  0.036  28.346
5   28       1   45000          650        30000        1  28|1|45000|650|30000  28|0|45000|650|30000-28|1|45000|650|30000       1        2  3500   127  0.036  28.346
In this example:

Each pair of rows represents a discriminatory pair where changing only the gender attribute (from 0 to 1 or vice versa) causes the model's prediction to change from 0 to 1 or 1 to 0.
The diff_outcome column shows the absolute difference in predictions (always 1 for binary classification).
The metrics columns (TSN, DSN, SUR, DSS) are the same for all rows, representing the overall algorithm performance.
The algorithm found 127 discriminatory instances out of 3500 tested samples (SUR = 0.036), with an average time of 28.346 seconds to find each discriminatory instance.